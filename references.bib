
@article{bing_supervised_2019,
	title = {Supervised Learning in {SNN} via Reward-Modulated Spike-Timing-Dependent Plasticity for a Target Reaching Vehicle},
	volume = {13},
	issn = {1662-5218},
	url = {https://www.frontiersin.org/article/10.3389/fnbot.2019.00018/full},
	doi = {10.3389/fnbot.2019.00018},
	pages = {18},
	journaltitle = {Frontiers in Neurorobotics},
	shortjournal = {Front. Neurorobot.},
	author = {Bing, Zhenshan and Baumann, Ivan and Jiang, Zhuangyi and Huang, Kai and Cai, Caixia and Knoll, Alois},
	urldate = {2022-03-31},
	date = {2019-05-03},
	file = {Full Text:/home/max/Zotero/storage/ISZP26ET/Bing et al. - 2019 - Supervised Learning in SNN via Reward-Modulated Sp.pdf:application/pdf},
}

@inproceedings{huang_optimizing_2017,
	title = {Optimizing the dynamics of spiking networks for decoding and control},
	doi = {10.23919/ACC.2017.7963374},
	abstract = {In this paper, an optimization-based approach to construct spiking networks for the purposes of decoding and control is presented. Specifically, we postulate a simple objective function wherein a network of interacting, primitive spiking units is decoded in order to drive a linear system along a prescribed trajectory. The units are assumed to spike only if doing so will decrease a specified objective function. The optimization gives rise to an emergent network of neurons with diffusive dynamics and a threshold-based spiking rule that bears resemblance to the Integrate and Fire neural model.},
	eventtitle = {2017 American Control Conference ({ACC})},
	pages = {2792--2798},
	booktitle = {2017 American Control Conference ({ACC})},
	author = {Huang, Fuqiang and Riehl, James and Ching, {ShiNung}},
	date = {2017-05},
	note = {{ISSN}: 2378-5861},
	keywords = {Biological neural networks, Control systems, Decoding, Linear programming, Linear systems, Neurons, Optimization},
	file = {IEEE Xplore Abstract Record:/home/max/Zotero/storage/GXAYJL4Y/7963374.html:text/html;IEEE Xplore Full Text PDF:/home/max/Zotero/storage/JQTNHXJB/Huang et al. - 2017 - Optimizing the dynamics of spiking networks for de.pdf:application/pdf},
}

@article{boerlin_predictive_2013,
	title = {Predictive Coding of Dynamical Variables in Balanced Spiking Networks},
	volume = {9},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003258},
	doi = {10.1371/journal.pcbi.1003258},
	abstract = {Two observations about the cortex have puzzled neuroscientists for a long time. First, neural responses are highly variable. Second, the level of excitation and inhibition received by each neuron is tightly balanced at all times. Here, we demonstrate that both properties are necessary consequences of neural networks that represent information efficiently in their spikes. We illustrate this insight with spiking networks that represent dynamical variables. Our approach is based on two assumptions: We assume that information about dynamical variables can be read out linearly from neural spike trains, and we assume that neurons only fire a spike if that improves the representation of the dynamical variables. Based on these assumptions, we derive a network of leaky integrate-and-fire neurons that is able to implement arbitrary linear dynamical systems. We show that the membrane voltage of the neurons is equivalent to a prediction error about a common population-level signal. Among other things, our approach allows us to construct an integrator network of spiking neurons that is robust against many perturbations. Most importantly, neural variability in our networks cannot be equated to noise. Despite exhibiting the same single unit properties as widely used population code models (e.g. tuning curves, Poisson distributed spike trains), balanced networks are orders of magnitudes more reliable. Our approach suggests that spikes do matter when considering how the brain computes, and that the reliability of cortical representations could have been strongly underestimated.},
	pages = {e1003258},
	number = {11},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Boerlin, Martin and Machens, Christian K. and Denève, Sophie},
	urldate = {2022-09-20},
	date = {2013-11-14},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Neurons, Action potentials, Dynamical systems, Membrane potential, Network analysis, Neural networks, Neuronal tuning, Sensory perception},
	file = {Full Text PDF:/home/max/Zotero/storage/PA8LXANX/Boerlin et al. - 2013 - Predictive Coding of Dynamical Variables in Balanc.pdf:application/pdf;Snapshot:/home/max/Zotero/storage/ACMFCVFE/article.html:text/html},
}

@article{brendel_learning_2020,
	title = {Learning to represent signals spike by spike},
	volume = {16},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007692},
	doi = {10.1371/journal.pcbi.1007692},
	abstract = {Networks based on coordinated spike coding can encode information with high efficiency in the spike trains of individual neurons. These networks exhibit single-neuron variability and tuning curves as typically observed in cortex, but paradoxically coincide with a precise, non-redundant spike-based population code. However, it has remained unclear whether the specific synaptic connectivities required in these networks can be learnt with local learning rules. Here, we show how to learn the required architecture. Using coding efficiency as an objective, we derive spike-timing-dependent learning rules for a recurrent neural network, and we provide exact solutions for the networks’ convergence to an optimal state. As a result, we deduce an entire network from its input distribution and a firing cost. After learning, basic biophysical quantities such as voltages, firing thresholds, excitation, inhibition, or spikes acquire precise functional interpretations.},
	pages = {e1007692},
	number = {3},
	journaltitle = {{PLOS} Computational Biology},
	shortjournal = {{PLOS} Computational Biology},
	author = {Brendel, Wieland and Bourdoukan, Ralph and Vertechi, Pietro and Machens, Christian K. and Denève, Sophie},
	urldate = {2022-09-20},
	date = {2020-03-16},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {Neurons, Action potentials, Membrane potential, Neural networks, Neuronal tuning, Coding mechanisms, Signaling networks, Speech signal processing},
	file = {Full Text PDF:/home/max/Zotero/storage/CZV96R6W/Brendel et al. - 2020 - Learning to represent signals spike by spike.pdf:application/pdf;Snapshot:/home/max/Zotero/storage/4QSKUIUN/article.html:text/html},
}

@online{noauthor_elsevier_nodate,
	title = {Elsevier Enhanced Reader},
	url = {https://reader.elsevier.com/reader/sd/pii/S0893608019303181?token=C6EE78C8DB33212369B121653DF2B3F9D33D1824C163EE4735825EDD496E32302EB459DD90100FEFE7959B6E5B19B438&originRegion=eu-west-1&originCreation=20220920145904},
	urldate = {2022-09-20},
	langid = {english},
	doi = {10.1016/j.neunet.2019.09.036},
	file = {Full Text:/home/max/Zotero/storage/SZC73ZSZ/Elsevier Enhanced Reader.pdf:application/pdf;Snapshot:/home/max/Zotero/storage/AE8SZ8GB/S0893608019303181.html:text/html},
}

@article{taherkhani_review_2020,
	title = {A review of learning in biologically plausible spiking neural networks},
	volume = {122},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608019303181},
	doi = {10.1016/j.neunet.2019.09.036},
	abstract = {Artificial neural networks have been used as a powerful processing tool in various areas such as pattern recognition, control, robotics, and bioinformatics. Their wide applicability has encouraged researchers to improve artificial neural networks by investigating the biological brain. Neurological research has significantly progressed in recent years and continues to reveal new characteristics of biological neurons. New technologies can now capture temporal changes in the internal activity of the brain in more detail and help clarify the relationship between brain activity and the perception of a given stimulus. This new knowledge has led to a new type of artificial neural network, the Spiking Neural Network ({SNN}), that draws more faithfully on biological properties to provide higher processing abilities. A review of recent developments in learning of spiking neurons is presented in this paper. First the biological background of {SNN} learning algorithms is reviewed. The important elements of a learning algorithm such as the neuron model, synaptic plasticity, information encoding and {SNN} topologies are then presented. Then, a critical review of the state-of-the-art learning algorithms for {SNNs} using single and multiple spikes is presented. Additionally, deep spiking neural networks are reviewed, and challenges and opportunities in the {SNN} field are discussed.},
	pages = {253--272},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Taherkhani, Aboozar and Belatreche, Ammar and Li, Yuhua and Cosma, Georgina and Maguire, Liam P. and {McGinnity}, T. M.},
	urldate = {2022-09-20},
	date = {2020-02-01},
	langid = {english},
	keywords = {Learning, Spiking neural network ({SNN}), Synaptic plasticity},
	file = {Full Text:/home/max/Zotero/storage/HKRNSUWU/Taherkhani et al. - 2020 - A review of learning in biologically plausible spi.pdf:application/pdf;ScienceDirect Snapshot:/home/max/Zotero/storage/CYGAQY8Z/S0893608019303181.html:text/html},
}

@article{zheng_introductory_2022,
	title = {An Introductory Review of Spiking Neural Network and Artificial Neural Network: From Biological Intelligence to Artificial Intelligence},
	url = {http://arxiv.org/abs/2204.07519},
	shorttitle = {An Introductory Review of Spiking Neural Network and Artificial Neural Network},
	abstract = {Recently, stemming from the rapid development of artificial intelligence, which has gained expansive success in pattern recognition, robotics, and bioinformatics, neuroscience is also gaining tremendous progress. A kind of spiking neural network with biological interpretability is gradually receiving wide attention, and this kind of neural network is also regarded as one of the directions toward general artificial intelligence. This review introduces the following sections, the biological background of spiking neurons and the theoretical basis, different neuronal models, the connectivity of neural circuits, the mainstream neural network learning mechanisms and network architectures, etc. This review hopes to attract different researchers and advance the development of brain-inspired intelligence and artificial intelligence.},
	journaltitle = {{arXiv}:2204.07519 [cs]},
	author = {Zheng, Shengjie and Qian, Lang and Li, Pingsheng and He, Chenggang and Qin, Xiaoqin and Li, Xiaojian},
	urldate = {2022-09-20},
	date = {2022-04-09},
	eprinttype = {arxiv},
	eprint = {2204.07519},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/home/max/Zotero/storage/3PER3Y9Y/Zheng et al. - 2022 - An Introductory Review of Spiking Neural Network a.pdf:application/pdf;arXiv.org Snapshot:/home/max/Zotero/storage/CZ94M8PX/2204.html:text/html},
}

@article{izhikevich_simple_2003,
	title = {Simple model of spiking neurons},
	volume = {14},
	issn = {1941-0093},
	doi = {10.1109/TNN.2003.820440},
	abstract = {A model is presented that reproduces spiking and bursting behavior of known types of cortical neurons. The model combines the biologically plausibility of Hodgkin-Huxley-type dynamics and the computational efficiency of integrate-and-fire neurons. Using this model, one can simulate tens of thousands of spiking cortical neurons in real time (1 ms resolution) using a desktop {PC}.},
	pages = {1569--1572},
	number = {6},
	journaltitle = {{IEEE} Transactions on Neural Networks},
	author = {Izhikevich, E.M.},
	date = {2003-11},
	note = {Conference Name: {IEEE} Transactions on Neural Networks},
	keywords = {Neurons, Bifurcation, Biological system modeling, Biology computing, Biomembranes, Brain modeling, Computational modeling, Large-scale systems, Mathematical analysis, Mathematical model},
	file = {Full Text:/home/max/Zotero/storage/86M9DHRU/Izhikevich - 2003 - Simple model of spiking neurons.pdf:application/pdf;IEEE Xplore Abstract Record:/home/max/Zotero/storage/VARGU5K2/1257420.html:text/html},
}

@book{johnston_foundations_1995,
	location = {Cambridge, Mass},
	title = {Foundations of cellular neurophysiology},
	isbn = {978-0-262-10053-3},
	pagetotal = {676},
	publisher = {{MIT} Press},
	author = {Johnston, Daniel and Wu, Samuel Miao-sin},
	date = {1995},
	keywords = {Neurons, Ion Channels, Neurophysiology, physiology, Synaptic Transmission},
}

@article{hodgkin_quantitative_1952,
	title = {A quantitative description of membrane current and its application to conduction and excitation in nerve},
	volume = {117},
	issn = {1469-7793},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.1952.sp004764},
	doi = {10.1113/jphysiol.1952.sp004764},
	pages = {500--544},
	number = {4},
	journaltitle = {The Journal of Physiology},
	author = {Hodgkin, A. L. and Huxley, A. F.},
	urldate = {2022-09-21},
	date = {1952},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1113/jphysiol.1952.sp004764},
	file = {Full Text PDF:/home/max/Zotero/storage/HZXTUGM9/Hodgkin and Huxley - 1952 - A quantitative description of membrane current and.pdf:application/pdf;Snapshot:/home/max/Zotero/storage/5SKVB6M9/jphysiol.1952.html:text/html},
}

@article{azevedo_equal_2009,
	title = {Equal numbers of neuronal and nonneuronal cells make the human brain an isometrically scaled-up primate brain},
	volume = {513},
	issn = {1096-9861},
	doi = {10.1002/cne.21974},
	abstract = {The human brain is often considered to be the most cognitively capable among mammalian brains and to be much larger than expected for a mammal of our body size. Although the number of neurons is generally assumed to be a determinant of computational power, and despite the widespread quotes that the human brain contains 100 billion neurons and ten times more glial cells, the absolute number of neurons and glial cells in the human brain remains unknown. Here we determine these numbers by using the isotropic fractionator and compare them with the expected values for a human-sized primate. We find that the adult male human brain contains on average 86.1 +/- 8.1 billion {NeuN}-positive cells ("neurons") and 84.6 +/- 9.8 billion {NeuN}-negative ("nonneuronal") cells. With only 19\% of all neurons located in the cerebral cortex, greater cortical size (representing 82\% of total brain mass) in humans compared with other primates does not reflect an increased relative number of cortical neurons. The ratios between glial cells and neurons in the human brain structures are similar to those found in other primates, and their numbers of cells match those expected for a primate of human proportions. These findings challenge the common view that humans stand out from other primates in their brain composition and indicate that, with regard to numbers of neuronal and nonneuronal cells, the human brain is an isometrically scaled-up primate brain.},
	pages = {532--541},
	number = {5},
	journaltitle = {The Journal of Comparative Neurology},
	shortjournal = {J Comp Neurol},
	author = {Azevedo, Frederico A. C. and Carvalho, Ludmila R. B. and Grinberg, Lea T. and Farfel, José Marcelo and Ferretti, Renata E. L. and Leite, Renata E. P. and Jacob Filho, Wilson and Lent, Roberto and Herculano-Houzel, Suzana},
	date = {2009-04-10},
	pmid = {19226510},
	keywords = {Aged, Antigens, Nuclear, Brain, Cerebral Cortex, Humans, Immunohistochemistry, Male, Middle Aged, Nerve Tissue Proteins, Neuroglia, Neurons},
}