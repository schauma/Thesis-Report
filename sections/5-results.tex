\chapter{Results}
Describe the results of the degree project.
Analyses of results

How does the optimal network simulate?
Influence of parameters\\
What are the magic parameters? mu etc



The control with optimal works fine too.
Influence of the parameters
Magic number tuning required for x,y,z etc

Learning works fine with magic numbers
Examples.
What works better?
What works worse?
What if I only learn 1 part? What is the bigger error.
Convergence over time usually very bad
Compared to the eigenvalues maybe convergence improves
How do the parameters look? Heatmap
How to judge the accuracy of the closeness of the parameters to the optimal
What is the influence of the input
You have to juggle the values of learning rate and input amplitude to reach ideal results



\section{Results on the Simulation from \cref{sec:res_simulation}}
\subsection{Toy Example}
\begin{figure}[h!]
	\centering
	\centering
	\includegraphics[width=\textwidth]{../../plots/Simulation/basic_example.pdf}
	\caption{Baseline example}
	\label{fig:sim_res_1}
\end{figure}

The results from simulating the network with given input $\bmu{c}(t)$ can be seen in \cref{fig:sim_res_1}. In this scalar example the ODE
\begin{equation}\label{eq:leaky_integration_example}
\dot{x} = -10x +c(t)
\end{equation}
is simulated with 2 neurons. One neuron corrects the network simulation of the system up and down respectively. As shown before this correction happens immediately after each spike by adding weights of the relevant decoding vector to the trajectory. Since this example is following a scalar variable with 2 neurons the decoding matrix $\bmu{\Gamma}\in \mathbb{R}^{1\times2}$ and set to $\left[0.09,-0.09\right]$ for this example. This can be seen in \cref{fig:sim_res_1}, the neural network simulation jumps up after each spike by 0.09 is added. The external input follows a linear increase until 0.15s after which it remains constant.\\
For reference a conventional numeric solution is given which lies directly between the two neurons threshold, visualized with dotted lines.\\
\begin{figure}[h!]
	\centering
	\begin{subfigure}[t]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../../plots/Simulation/small_lambdaD.pdf}
		\caption{Reduced Readout Decay rate}
		\label{fig:sim_low_lambda}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../../plots/Simulation/small_gamma.pdf}
		\caption{Reduced Threshold}
		\label{fig:sim_low_gamma}
	\end{subfigure}
	\caption{Variation of Readout Decay and Decoder for simple 1D system.}
	\label{fig:sim_res_2}
\end{figure}
Already for this toy example different parameters can be tuned to get different results. In \cref{fig:sim_low_lambda} the readout decay is reduced compared to \cref{fig:sim_res_1} which reduces how fast the output tends to zero. This also elevates the importance of a single spike as it has longer lasting effects on the output, seen by the system showing fewer spikes than before.\\
Alternatively, if the decoding weights can be scaled to let each spike make a smaller change in the output seen in \cref{fig:sim_low_gamma}. Since the threshold is closely tied to the Decoding weights this also reduces the spike threshold and therefore yields more accurate results.\\

\subsection{Toy example in 2D}

\begin{figure}[h!]
	\centering
	\includegraphics[width = \textwidth]{../../plots/Simulation/simple_2d.pdf}
	\caption{Simple 2D example with numerical solution and spike response. Curves for $x$ in yellow/blue. Curves for $y$ in purple/red. The networks output closely tracks the perfect numerical solution. For the network each decoding vector was chosen from a normal distribution and normalized to $\left\lVert\bmu{\Gamma}_i\right\rVert_2 =0.3$. Beneath a raster plot for each spiking neuron \\ On the left the threshold for each neuron's projected error.}
	\label{fig:sim_res_simple}
\end{figure}



\subsection{Geometry in 2D}
\begin{figure}[h!]
	\centering
	\includegraphics[width = \textwidth]{../../plots/Simulation/2d_simple_spikes.pdf}
	\caption{Simple 2D example with numerical solution and spike response. Curves for $x$ in yellow/blue. Curves for $y$ in purple/red. The networks output closely tracks the perfect numerical solution. For the network each decoding vector was chosen from a normal distribution and normalized to $\left\lVert\bmu{\Gamma}_i\right\rVert_2 =0.3$. Beneath a raster plot for each spiking neuron \\ On the left the threshold for each neuron's projected error.}
	\label{fig:sim_res_geometric}
\end{figure}
In two dimensions the network with its allows for a geometric interpretation. For this we let the network simulate a simple leaky integration of inputs as in \cref{eq:leaky_integration_example} but in two dimensions. We have the corresponding results in \cref{fig:sim_res_geometric} on the right. On the left we a phase plot in the $xy$-axis.


\subsection{Importance of Feedforward/Decoding weights}
So far we have not given much attention to decoding/feedforward weights. Yet they play a crucial rule in the performance of our network as seen in \cref{fig:sim_res_3}. Here we simulate again a simple leaky integration as in \cref{eq:leaky_integration_example} however this time in 2D. With the output, in each test we also plot a bounding box for the relative error.\\
The bounding box of the error is the normal to each of the decoding weights. In \cref{eq:x_hat} the network output gets the $i$th column $\bmu{\Gamma}$ added when the $i$th neuron spikes. From the minimization of the cost we know that the spike of neuron $i$ reduces the error in projected by $\bmu{\Gamma}_i$. If we know imagine the origin of the \cref{fig:sim_res_3} l to always be on top of the true trajectory of $\left[x ,\ y\right]^T$, the network's error is just deviation from the origin.

Explain phase plot
explain the normal to the decoding weights
Explain the 3 plots
Dont forget to explain the Trichter in the bad plots
\begin{figure}[h!]
		\centering
	\begin{subfigure}[t]{0.6\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../../plots/Simulation/2d_perfect_gamma.pdf}
		\caption{Network }
		\label{fig:sim_perfect_gamma}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.6\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../../plots/Simulation/2d_random_gamma.pdf}
		\caption{Reduced Threshold}
		\label{fig:sim_random_gamma}
	\end{subfigure}
	\hfill
\begin{subfigure}[t]{0.6\textwidth}
	\centering
	\includegraphics[width=\textwidth]{../../plots/Simulation/2d_bad_gamma.pdf}
	\caption{Reduced Threshold}
	\label{fig:sim_bad_gamma}
\end{subfigure}
	\caption{Network output after leaky integration of oscillating input with different decoding vectors.}
	\label{fig:sim_res_3}
\end{figure}


\subsection{Bigger Systems}
\begin{figure}[h!]
	\centering
		\includegraphics[width=\textwidth]{../../plots/Simulation/big_system.pdf}
	\caption{Trajectory of dynamic mass spring system with 100 masses and 2 thousand neurons. Only the first 3 trajectories are plotted and the spikes for the first 200 neurons.	}
	\label{fig:big_systems}
\end{figure}

So far we have only looked simple systems in 1D or 2D. However our network can also simulate more complex higher dimensional systems.
In the example in \cref{fig:big_systems} we simulated a linear n-mass spring system with the dynamics
\begin{equation}
	m_i\ddot{x}_i = K\cdot \begin{bmatrix}
	1&-2&1
	\end{bmatrix} \cdot	\begin{bmatrix}
	x_{i-1}\\
	x_i\\
	x_{i+1}
	\end{bmatrix} + c_i(t).
\end{equation}
The external forces were each offset sinusoidal waves with varying frequency and amplitude for the first 3 masses and random forces for the rest. As can be seen from the figure, the network perfectly overlays the numerical solution.


\subsection{Varying cost parameters $\mu,\nu$}
\begin{figure}[h!]
	\centering
	\begin{subfigure}[t]{0.6\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../../plots/Simulation/2d_cost_none.pdf}
		\caption{Network }
		\label{fig:sim_no_cost}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.6\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../../plots/Simulation/2d_cost_nu.pdf}
		\caption{Reduced Threshold}
		\label{fig:sim_nu_cost}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.6\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../../plots/Simulation/2d_cost_mu.pdf}
		\caption{Reduced Threshold}
		\label{fig:sim_mu_cost}
	\end{subfigure}
	\caption{Network output after leaky integration of oscillating input with different decoding vectors.}
	\label{fig:sim_res_4}
\end{figure}

\section{Results on the control}
For the evaluation of the results for the control problem derived in \cref{sec:control} we first note a few important implementational details in order to get useful results. \\
Afterwards we compare the performance of the two different derivations from \cref{ssec:control_dynamics,ssec:extension} for specific examples as well as their limits.\todo{For example dirac doesnt work,Actually anything with a discontinuity doesnt work???,long simulation times. All for the one with  error.}
Lastly we take a closer look at the main reason for this system to work and how we can adopt it to our learning problem in \cref{sec:learning}.
\todo{Mention that the rate stuff is kind of useless unless you want to reduce the number of spikes}
\subsection{Implementation details}
Before any simulation can be run it is important that certain detail in the implementation are set in the correct order for the results to make sense.


\subsubsection{Multiple spikes per timestep}
Although this also applies for the simulation step in \cref{sec:res_simulation}, its affects are much more pronounced in the control setting.\\
\paragraph{Single Spike}
As the dynamics outlined suggest the simulation step includes a check if a threshold has been reached. \\
A simple implementation in MatLab pseudo code is seen in \cref{lst:single_spike}.

\begin{lstlisting}[language=Matlab, caption=Single spike implementation,label=lst:single_spike]

for t_step = 1:N_step
	V = update_Voltages(V,dt,Ws,Wf,c,...);
	[value, index] = max(V-Threshold);
	spikes(index,t_step) = 1;
	V = V + Wf(:,k);
	...
end
\end{lstlisting}
While this works for a variety of problems it does not give results for any given system and reference trajectory.\\
Especially problems with an abrupt change underperform since the network is only allowed to correct the error by one spike. However by jumps and rapid changes this is not sufficient to compensate errors and then networks struggles for many iterations to recover, ruining the overall accuracy in the process.\\
\paragraph{Parallel Spikes}
Now the direct way to fix this to find all neurons that have reached their thresholds. Since networks have many neurons, letting every neuron spike increases the networks ability to correct errors. A potential implementation is seen in \cref{lst:multi_spike}.
\begin{lstlisting}[language=Matlab, caption=Letting every neuron spike in parallel,label=lst:multi_spike]

for t_step = 1:N_step
	V = update_Voltages(V,dt,Ws,Wf,c,...);
	spiking_neurons = V>Threshold; % procudes a list
	spikes(spiking_neurons,t_step) = 1;
	V = V + Wf(:,spiking_neurons);
	...
end
\end{lstlisting}
The problem with this implementation is that the error gets tracked by multiple neurons simultaneously and also in both in positive and negative direction. To illustrate this we consider a simple example where we have a single control variable $u$ and $2N$ neurons, where $N$ neurons track positive and negative error respectively.\\
Disregarding noise, the threshold is reached by $N$ neurons synchronously. As long as the total error is larger than the compensation of $N$ neurons in the first place, this works just fine. However this is not permanent and at some point all $N$ neurons firing overly depolarize the opposite side neurons to the extend that they all fire in the next iteration regardless of the real system error.\\
The root cause is that a single spike influences the whole network. In this extreme case 1 spike resets all other $N-1$ neurons that were spiking as well and therefore would not give any performance boost compared to the previous approach.\\
The network with this implementation behaves normally while a rapid change is present but will evolve into $N$ neurons spiking in alternating cadence respectively.\\
To remedy this it is necessary that one dimension of the error is only projected onto two neurons. While this can be achieved by carefully selecting the decoder it is not a generic approach. Moreover this reverses the idea of multiple neurons tracking the error in order to increase the networks performance and would result in zero change.\\
Therefore we need to let a single neuron spike multiple times.

\paragraph{Multiple Spikes}
The correct way to handle this problem is to allow more than one spike per timestep but compute each spike's change in the network separately. This way we can achieve the necessary performance without the problems of the previous approach. A implementation of such a regime is seen in \cref{lst:correct_spike}

\begin{lstlisting}[language=Matlab, caption=Letting each neurons spike as many times a necessary while computing each spike's influence sequentially.,label=lst:correct_spike]

for t_step = 1:N_step
	V = update_Voltages(V,dt,Ws,Wf,c,...);
	[value, index] = max(V-Threshold);
	while value > 0
		spikes(index,t_step) = 1;
		V = V + Wf(:,k);
		...
		[value, index] = max(V-Threshold);
	end
end
\end{lstlisting}

\subsubsection{Signed Error and Slow Connectivity}
\paragraph{Neglected Term}
First to note is that the derivation of $\bmu{W}^s$  in \cref{eq:Ws_wrong} is different from the definition it \cite{huang_optimizing_2017} in two ways. Firstly in the original derivation the rate is scaled by $\lambda_d$ i.e.
\begin{equation}
	\hat{r}(t) = \lambda_dr(t).
\end{equation}.
Substituting this in for the equations in \cref{ssec:control_dynamics} gives the exact same dynamics.\\
Secondly and more importantly in their derivation consider network in the limit for $N\rightarrow \infty$ neurons. Specifically the derivation arrives at
\begin{equation}
\begin{aligned}
\dot{\bmu{V}}(t)= & \bmu{\Omega}^T \bmu{B}^T\left(\dot{\hat{\bmu{x}}}(t)-\dot{\bmu{x}}(t)\right)-\mu \lambda_d \dot{\bmu{r}}(t) \\
= & \bmu{\Omega}^T \bmu{B}^T \bmu{A} \bmu{L} \bmu{V}(t) \\
& +\left(\mu \lambda_d \bmu{\Omega}^T \bmu{B}^T \bmu{A} \bmu{L}+\mu \lambda_d^2 \bmu{I}-\frac{1}{\lambda_d} \bmu{\Omega}^T \bmu{B}^T \bmu{B} \bmu{\Gamma}\right) \bmu{r}(t) \\
& -\left(\bmu{\Omega}^T \bmu{B}^T \bmu{B} \bmu{\Omega}+\mu \lambda_d^2 \bmu{I}\right) \bmu{o}(t) \\
& +\bmu{\Omega}^T \bmu{B}^T(-\bmu{A} \hat{\bmu{x}}(t)+\dot{\hat{\bmu{x}}}(t))
\end{aligned}
\end{equation}
where $$\bmu{L} = \left(\bmu{B\Omega\Omega}^T\bmu{B}^T\right)^{âˆ’1}\bmu{B\Omega}$$ is the pseudo-inverse of $\left(\bmu{B\Omega}\right)^T$. Our focus is now set on the terms in front of the rate. As \cite{huang_optimizing_2017} and the original derivation in \cite{boerlin_predictive_2013} argue the term $\mu\lambda_d^2$ vanishes in the limit and can therefore be neglected, yielding the above result for $\bmu{W}^s$ above in \cref{eq:Ws_wrong}.\\
However in the testing it was noticed that this derivation only works acceptable for a small range of values for $\lambda_d$, explicitly $\lambda_d \leq 20$. For values above the performance deteriorates rapidly if not a large number of neurons is considered.\\
After testing the relative importance of terms it was noted that the term $\mu\lambda_d^2$ does improve the performance with a smaller number of neurons significantly compared to the other terms and was therefore added in the results following below.\\
Furthermore the authors give later examples that show their use of the extra term.
This can be seen in all examples but especially in example figure 2. Conveniently the authors provided a picture of the their $\bmu{W}^s$ matrix which clearly shows a nonzero entry on the main diagonal. The example showcases the case $\bmu{\Gamma} = \bmu{0}$ which, given the definition of $\bmu{W}^s$ in \cref{eq:Ws_wrong}, yields $\bmu{W}^s = \bmu{0}$. Examining the colormap reveals that the values on the diagonal equal to 1 which is the same $\mu\lambda_d^2$ in the given example. The same can be validated for the other examples.\\
Running the same example results in a subpar performance shown in \cref{fig:bad_Ws}. To match the behaviour shown in the paper we needed to either involve the omitted term or increase the number of neurons to $N= 500$. Even though in \cref{fig:bad_Ws_compensated} it might look like the behaviour is similar it is important to point out that over time the network's performance deteriorates over time similar to \cref{fig:bad_Ws} and certainly cannot match the behaviour shown in \cite{huang_optimizing_2017}.
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{../../plots/Control/controller_without_extra_term.pdf}
	\caption{Control performance of the same the example in figure 2 from \cite{huang_optimizing_2017} without the extra $\lambda_d\mu^2\bmu{I}$ term. Simulation with $N=100$ neurons.}
	\label{fig:bad_Ws}
\end{figure}


\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{../../plots/Control/controller_without_extra_term_compensation.pdf}
	\caption{Control performance of the same the example in figure 2 from \cite{huang_optimizing_2017} without the extra $\mu\lambda_d^2\bmu{I}$ but $N=500$ neurons to compensate.Neuron are split 50:50 as before.}
	\label{fig:bad_Ws_compensated}
\end{figure}



Table
Mit extra term
ohne extra term
 Fehler in einem beispiel mit extra
 anzahl neuron die man hinzufuegen muss um ausgleich zu haben


\paragraph{Error sign}
The second important implementational detail is that concerns the error term in \cref{eq:direct_error_voltage} and the corresponding plots shown in \cite{huang_spiking_2019}. Using the derivation found therein it was not possible to reproduce the results. After investigating the problem was found to be in the sign of the error term. To illustrate this it is useful to consider their basic example of a scalar system
\begin{equation}
	\dot{x} = -10 x + u\\
\end{equation}
but only two neurons with weights
\begin{equation}
\begin{aligned}
\bmu{\Omega} &= c\cdot\left[-1 , 1\right] \quad c>0\\
\bmu{\Gamma} &= \left[0,0\right]
\end{aligned}
\end{equation}
and no additional cost terms.\\
This configuration allows for a straightforward allocation of functions to individual neurons. In the provided illustration, Neuron 2's voltage tracks the error whenever the network output $\hat{x}$ lags behind the reference value $x$. It becomes active if the deviation surpasses
\begin{equation}
	V_2 = 1c\left(x-\hat{x} \right) < \frac{c^2}{2}
\end{equation}
prompting a corrective response to increase the network output.
We now consider the error term with $e(t) = x-\hat{x}$
\begin{equation}
\begin{aligned}
	\bmu{\dot{V}}&= \bmu{\Omega B}^T \bmu{A}e(t) + \dots\\
	\bmu{\dot{V}}&=  -10c \cdot\begin{bmatrix}-1\\1\end{bmatrix}e(t).
\end{aligned}
\end{equation}
Now, let's examine a scenario where the network is falling short of the reference value, leading us to deduce that $e(t)>0$. Consequently, we can conclude that the error term is conflicting with the intended definition of our voltage by inadvertently increasing the voltage of Neuron 1 in the wrong direction while it is Neuron 2 that should increase its voltage.\\
One way to resolve this issue is to add an artificial minus sign the error. However this only brings changes the problem to the case when $\bmu{A} = 10$. The other solution is to interpret the phrasing of the original paper literally and assume $\bmu{A}$ as a literal "gain" which therefore is usually greater than zero and take $|A|$ for the computation, yielding proper results. How either of these additions could emerge out of the derivation is so far unclear.\todo{Double check the example and verify it. Additionally maybe ask arvind about the whole thing.}\\
Only with either of these additions it was possible to reproduce the results show in the paper \cite{huang_spiking_2019}.\\
However it is interesting to note that in different test scenarios it was found that the error term contributes only a small amount to the voltage in \cref{eq:direct_error_voltage} and therefore could be omitted in many cases directly.
This holds also true for the example here where the omitted error makes the network less accurate but still usable compared to the method introduced in the original work which diverges rapidly.\\

\paragraph{Output Feedback}
How can I understand the output feedback? ASK ARVIND
Useless Output feedback???

Adjustment for the Outputfeedback, even though it is useless

If you have output feedback you still need to give the whole state to the network
so even though you have output, same goes for the derivative. So it is just useful if you want to give some output but not you know the whole system and the state so you can control. Which kind of makes it a bit useless.\\
Because this formulation does not allow e to be the error between network and reference. It has to be error between states. So my controller is just a greedy P controller?


\subsection{Numerical treatment of spikes? Aka scale by 1/dt}
Do it for the control signal but also for the reference signal (when there are jumps in the derivative for example)
\subsection{Performance Comparison}
Test1: kind of step function 1d with a couple neurons
Test2: many dimensions random neuron decoders and some oscilating ref
Accuracy
\# spikes
Looks like they perform identical???


\subsection{Limits}
prob noise in the inputs?
noise in the network
B'C' = 0 bereits klar
if there is a limit of how many spikes per time interval the magnitude is bound
size?? how to find a large controllable system?
inexact derivatives?




\subsection{Direct error/Feed-Forward}
The most important insight of the approach in \cref{eq:feeback_c,eq:feedback_c2} is that the signal $\bmu{c}(t)$ is the principal source of control in the system. Moreover it is explicitly calculated from the system $\bmu{A}$ as well as the information of the reference signal and its derivative.\\
If one considers the output feedback once more and where one does not have access to the target state $\bmu{x}$ but only its output $\bmu{y} = \bmu{Cx}$, the derivation breaks down. Instead the feed-forward input $ \bmu{c}$ would need to be defined in terms of $\bmu{y}$ and $\bmu{CA\hat{x}}$. This would eliminate the error term outright as it would now be implicitly part of $\bmu{c}$. The derivation would be similar
\begin{equation}
\begin{aligned}
\bmu{V} &= \bmu{\Omega}^T\bmu{B}^T\bmu{C}^T\left(\bmu{y} - \bmu{\hat{y}}\right) - \lambda_d\mu \bmu{r}\\
\bmu{\dot{V}} &= \bmu{\Omega}^T\bmu{B}^T\bmu{C}^T\left(\bmu{\dot{y}} - \bmu{\dot{\hat{y}}}\right) - \lambda_d\mu \bmu{\dot{r}}\\
&= \bmu{\Omega}^T\bmu{B}^T\bmu{C}^T\left(\bmu{\dot{y}} - \bmu{C}\bmu{\dot{\hat{x}}}\right) - \lambda_d\mu \bmu{\dot{r}}\\
&= \bmu{\Omega}^T\bmu{B}^T\bmu{C}^T\left(\bmu{\dot{y}} - \bmu{C}(\bmu{A}\bmu{\hat{x}} + \bmu{B}\bmu{u})\right) - \lambda_d\mu \bmu{\dot{r}}\\
&= \bmu{\Omega}^T\bmu{B}^T\bmu{C}^T\left(\bmu{\dot{y}} - \bmu{C}(\bmu{A}\bmu{\hat{x}} + \bmu{B}\bmu{u})\right) - \mu \lambda_d\left(-\lambda_d\bmu{r} + \lambda_d\bmu{o}\right)\\
&= \bmu{\Omega}^T\bmu{B}^T\bmu{C}^T\left(\bmu{\dot{y}} - \bmu{C}(\bmu{A}\bmu{\hat{x}} + \bmu{B}\left(\frac{1}{\lambda_d}\bmu{\Gamma} \bmu{r} + \bmu{\Omega} \bmu{o}\right)\right) - \lambda_d^2\mu \left(-\bmu{r} + \bmu{o}\right)\\
&= \bmu{\Omega}^T\bmu{B}^T\bmu{C}^T\underbrace{\left(\bmu{\dot{y}} - \bmu{CA\hat{x}}\right)}_{\bmu{c}} + \left(\frac{1}{\lambda_d}\bmu{\Omega}^T\bmu{B}^T\bmu{C}^T\bmu{CB\Gamma} + \mu\lambda_d^2\bmu{I} \right)\bmu{r} \nonumber \\
&\phantom{{}=\bmu{\Omega}^T\bmu{B}^T\bmu{C}^T\underbrace{\left(\bmu{\dot{y}} - \bmu{CA\hat{x}}\right)}_{\bmu{c}}} - \left(\bmu{\Omega}^T\bmu{B}^T\bmu{C}^T\bmu{CB\Omega} - \mu\lambda_d^2\bmu{I} \right)\bmu{o}
\end{aligned}
\end{equation}
with the exception that is now the difference of the reference output and the network output instead of the the state. Otherwise the equation is identical to the original derivation. \todo{Should I keep this in? If so where should I put it?}\\

All this supports the idea that the feed-forward inputs are the governing force in the network's performance. Furthermore by the removal of the trajectory and only the use of its derivative show more the characteristics of an open loop PD controller.\todo{Is that really so?}\\
The benefit of this approach is that given that using this approach we do not need to know the state in order to track any trajectories but just the output or more the derivative of the output\\
that we dont know the state\\
that the C is not square


\section{Results on the learning}


To give a complete examination of results, the rigorous testing required to demonstrate them is not possible. Therefore we restrict our testing to select model problems and only demonstrate a selection of interesting results. Meanwhile testing has been conducted with more and different settings than presented here which will be mentioned occasionally for the purpose of completeness.\\
For our result evaluations we give a simple test scenario, with which we then evaluate the learning performance for slow and fast learning and combined learning separately. Slow learning alone is skipped as \cite{bourdoukan_enforcing_2015} expect to use it in conjunction to the fast learning. Furthermore, while testing it has been observed that the reliability and performance lack behind when only training is trained.\\
For the combined training we set out to find the determining  source of error in overall performance. To access we look into different metrics to determine the progress of learning and highlight key parameters for the success of learning the given system, e.g hyper-parameters or learning input sequences. Lastly we describe configurations where the learning breaks down.\\

\subsection{Test cases}
\subsubsection{Model problem}
As the first problem we consider a simple 2D toy example given by a damped oscillator of the form
\begin{equation}\label{eq:toy_example}
	\bmu{\dot{x}} = \begin{bmatrix}
	0&1\\ -1 &-10
	\end{bmatrix}\bmu{x} + \bmu{c}(t).
\end{equation}
Since we are only concerned with the learning and neglect the control at this point we give a predefined input $\bmu{c}(t)$ set as a single square impulse. With this trajectory we can observe the systems response to a sudden jump as well as the evolution of the system in the absence of external input. This allows us to see the system's performance in both scenarios of the control problem later on.


\subsubsection{Testing}
To measure the learning performance, we consider different metrics in order to get a complete picture.\\
Due to the analytical solution, we are in the fortunate position to compare the learning values to the optimal values. Yet different measures of performance can be evaluated. The most important and meaningful measure to consider is the network error after learning in a simulation.\\
As a reference, we plot the optimal solution of our toy example here.
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../../plots/Learning/reference_sol.pdf}
	\caption{Simulation of our toy example with the given external input c. The network simulation perfectly overlaps with the analytical solution. Maximum error in this example is $<0.04$.}
	\label{fig:reference_sol}
\end{figure}
As can be seen from \cref{fig:reference_sol} the network perfectly follows simulates the system's behaviour.\\
However also other measurements can be obtained to assess the networks learning. Alternatively, one can consider the convergence of the matrix parameters to their optimal values. Therefore, we measure the maximum relative deviation from the optimal value as
\begin{equation}\label{eq:largest_rel_dev}
	\Delta^\text{rel} \bmu{W}_{ij} =\left|1 - \frac{\bmu{W}^{\text{learned}}_{ij}}{\bmu{W}^\text{ex}_{ij}}\right|
\end{equation}
for each parameter. From there we can measure the maximum relative deviation or compute the sum of of deviations to access the parameter convergence.\\
Alternatively, since we are working on matrices, we look at the convergence of eigenvalues.\\
Knowing the optimal values to
\begin{equation}
\begin{aligned}
	\bmu{W}^f &= -\left(\bmu{\Gamma}^T\bmu{\Gamma} + \mu \bmu{I}\right)\\
	\bmu{W}^s &= \bmu{\Gamma}^T\left(\bmu{A} + \lambda_d\bmu{I}\right)\bmu{\Gamma}\\
\end{aligned}
\end{equation}
the eigenvalues can be computed.\\
For $\bmu{W}^f$ we only need to look at the eigenvalues of $\bmu{\Gamma}^T\bmu{\Gamma}$ and shift them by $\mu$. $\bmu{\Gamma}^T\bmu{\Gamma}$ itself only has $J$ non-zero eigenvalues $\text{rank}(\bmu{\Gamma}^T\bmu{\Gamma}) = J$ since $\text{rank}\left(
\bmu{\Gamma}\right) = J$.\\
For $\bmu{W}^s$, only J eigenvalues are nonzero since again the $\text{rank}(\bmu{\Gamma}) = J$.\\
Thus, we access the learning progress by measuring the distance of the learned eigenvalues compared to the true eigenvalues.\\

\subsection{Only learning of $\bmu{W}^f$}\label{ssec:Wf_res}
In order to evaluate the learning performance individually, we first consider each learning algorithm separately. Later, we use both to access the overall performance.\\
This also allows us to see the effects of changing learning parameters individually. To isolate the learning of the fast weights, we set the learning rate for the slow weights to zero and initialize $\bmu{W}^s$ to the optimal values from the analytic solution given in \cref{sec:simulation}. This provides optimal conditions for the learning algorithm.\\
We start first by looking at the convergence over learning before looking at errors in order to see whether any of the aforementioned measurements indeed give a meaningful insight into the networks performance.\\

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../../plots/Learning/wf_analyses.pdf}
	\caption{Learning of $\bmu{W}^f$ matrix and the end of the epoch. Top panel shows the movement of the two non-zero eigenvalues of $\bmu{W}^f$. The middle panel shows the largest relative deviation of $\bmu{W}^f$ calculated from \cref{eq:largest_rel_dev}. The bottom panel shows the maximal error during simulation between the learned and the optimal matrix.}
	\label{fig:wf_analyses}
\end{figure}

The learning performance of $\bmu{W}^f$ is seen in \cref{fig:wf_analyses}. As can be seen, the two non-zero eigenvalues of the matrix converge close to the
optimal values which are reached after $\approx 130$ epochs and keep hovering around.
We also see that the largest relative deviation is following the same trend in the beginning, but instead of settling down at $\approx 130$ it starts jumping around. The plot only prints the absolute relative deviation, it is interesting to check the signs during learning. While the error decreases from $-1$, the starting value if $\bmu{W}^f$ is initialized to zero, the jumping around after epoch $\approx 130$ only jumps in the positive direction.\\
Before the jumps the learning reaches begins, the learning algorithm manages to get the largest deviation less then $9.5\%$ away from the optimal value. Using this set of learned parameters the maximum absolute error during that simulation was $0.1187$ which corresponds to a relative error of $<4\%$.\\
The overall best performance according to our measures was reached in epoch 724 but because of the rapid fluctuations it can be assumed that the parameter set was found randomly. Interestingly did this not also give the best error results but an relative error of more than $10\%$. The best overall performance was in epoch 1630 with an relative error of $<3\%$. The largest relative deviation of the matrix  in that epoch was $\approx 12\%$ from the optimal value.\\
\begin{figure}
	\centering
	\begin{subfigure}[t]{0.48\textwidth}
		\includegraphics[width=\textwidth]{../../plots/Learning/wf_1630.pdf}
		\caption{$\bmu{W}^f$ matrix after 724 epochs of learning. Displayed are the parameter differences $\bmu{W}^f - \bmu{W}^f_{\text{exact}}$ }  % Add a caption for Subfigure 1
		\label{fig:subfig1}  % Add a label for referencing Subfigure 1
	\end{subfigure}
	\hspace{0.02\textwidth}  % Horizontal space between subfigures
	\begin{subfigure}[t]{0.48\textwidth}
		\includegraphics[width=\textwidth]{../../plots/Learning/wf_724.pdf}
		\subcaption{$\bmu{W}^f$ matrix after 1640 epochs of learning.}  % Add a caption for Subfigure 2
		\label{fig:subfig2}  % Add a label for referencing Subfigure 2
	\end{subfigure}
\caption{Matrix' learning progress for the best error performance (\subref{fig:subfig1}) and best relative deviation (\subref{fig:subfig2}). Learned was with different smoothed Gaussian noise for each epoch for $10^5$ timesteps.}
\label{fig:epoch_viz}
\end{figure}
Visualizing the matrices' differences from the optimal weights  of both of these epochs in \cref{fig:epoch_viz} it is apparent to see that the columns $i = [1,\dots,15] \cup [42,\dots,50]$  are learned slightly more accurately. These correspond to the reset of voltages of neurons $i$. The reason this is important is that for our test example these neurons are by far the most active. Thus by learning these neurons accurately while other weights are further apart from the optimal value gives better results for our example but worse results when looking at the largest deviation.\\
A possible reason for this behaviour is that the weights are to much dependent on the input of the learning input sequence that is used for learning in the current epoch. This will be investigated in a later section.\\
But this also means that neither the eigenvalues nor the largest relative deviation give reliable ways to measure our performance without looking at the simulation error. Also other measures like the sum of all deviations do not give a good account on the learning progress. Moreover, all these measure rely on the information given by the optimal weights, which in general is not available  information. From this plot it becomes clear hard to predict the network performance without actually testing it. While the eigenvalues or the largest relative deviation might indicate a good set of parameters the error can still make the whole simulation unusable as the error climbs as high as the original simulation in \cref{fig:reference_sol}. Therefore it is either necessary to find a better measure or avoid the oscillating effect by adjusting the learning parameters. Latter will be tested in the next section.\\
Lastly it is interesting to note that for all tests in which the the results were not completely unusable the the biggest error was after the stimulus $\bmu{c}$ was absent. Furthermore, the error crept up over time and the maximum error was often found towards the end of the simulation. This hints that the network's accuracy is deteriorating mostly when it is evolving on its own.\\

\todo{Therefore a learning limiting factor that if the change is less than a specific value stop learning. Put in the end!}
\todo{Mention sim parameters somewhere!}
\subsubsection{Parameter analyses}
\paragraph{$\beta$}
The prime parameter for the learning of $\bmu{W}^f$ is the parameter $\beta$. With the same learning and simulation setup as above we test different values of $\beta$. Empirically, we know that values outside the range of $(0.50,0.54)$ do not perform well enough to compare or do not converge outright. For the learning the identical input sequences were used.\\
To illustrate the influence of $\beta$ we run the simulation with a value close to $0.50$, close to the upper limit and an the best empirically found value.\\
In \cref{fig:beta_study} it becomes visible that with larger $\beta$ the rapid oscillations are reduced. However with $\beta = 0.54$ in the orange line the error is prohibitively high. The eigenvalues are not shown as they all follow a very similar convergence path as seen in \cref{fig:wf_analyses}.\\
The best performance was found empirically at $\beta = 0.522$ with comparatively good error measures while having few rapid jumps.\\
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../../plots/Learning/matrix_data_analyses_beta.pdf}
	\caption{Learning of toy example \cref{eq:toy_example} with different $\beta$. Top panel shows the relative deviation and bottom panel the absolute error.}
	\label{fig:beta_study}
\end{figure}

It is interesting to see that for different $\beta$ sudden changes in the error seem to happen after the same training epochs, indicating that the input sequence for this epoch influenced the learning enormously. This can be seen the easiest between epoch $7-180$ where spikes can be observed for each $\beta$. This indicates that the parameter changes learned in that epoch are to dominant for the whole network and the learning rate must be adjusted.\\
Unfortunately, none of our previously discussed measures is able to detect these anomalies, leaving us again with no other option that explicit testing of the network in simulation.\\
To mitigate this either the input sequences need to be modified of the learning rate adjusted. Latter will be the topic of the next section.\\


\paragraph{Learning rate}
Adjusting the learning rate for $\bmu{W}^f$ is an easy way to damp the oscillations seen in the previous plots, though coming with an increased computational cost. For comparison, we ran the learning algorithm with different learning rates to compare cost to performance in \cref{fig:lr_fast_study}. As expected reduces the learning rate the oscillations. While the oscillations still occur with the lower learning rate, its magnitude is greatly reduced. With the smallest tested learning rate $\alpha_f = 0.0001$ the noticeable oscillations occur after $\approx 2500$ epochs. The downside to that is just to reach useable results with an relative error of less than $10\%$ learning must continue for more than $1000$ epochs. In addition this is for a small system while we can expect even longer learning times for bigger systems.\\
On the other hand, the largest learning exhibits the largest error as well as enormous error variability. The learning rate for \cref{fig:beta_study} was double of the one used in \cref{fig:lr_fast_study}. However it is apparent that already in the first epoch the error reduces significantly.\\
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../../plots/Learning/Wf_lr_study.pdf}
	\caption{Learning of toy example \cref{eq:toy_example} with different learning rates $\alpha^f$. Top panel shows the relative deviation and bottom panel the absolute error.}
	\label{fig:lr_fast_study}
\end{figure}
To combine the benefits of both approaches in \cref{fig:adaptive_lr} we see gradually lowered the the learning rate after each epoch. We start with the previously used learning rate $\alpha_0^f = 0.005$ and gradually lower it to $\alpha_M^f = 0.0001$ over the course of $1000$ epochs.\\
For the reduction a value $p\in \left[0,1\right]$ was chosen beforehand and the learning rate for the next epoch $m$ calculated as
\begin{equation}\label{eq:drop}
	\alpha_{m}^f = (\left(1-p\right)^m + 0.02)\cdot\alpha_0^f,
\end{equation}
essentially allowing for and exponential decay of the learning rate with a backstop to avoid too small learning rates.\\
In \cref{fig:dr_fast_study} learning results for different $p$ are displayed. As can be seen, all variations perform more reliable than the any approach before as well as reaching comparable performance with fewer epochs. Furthermore, the fast drop rate of $p = 0.09$ appears to give the more consistent results. The improvement of performance is not reflected when looking at the largest deviation, though with the reduced oscillations thanks to the parameter tuning the eigenvalues allow for a more accurate description of the learning status. While there convergence to the true values does not occur, the convergence to a value itself can be seen as the completion of learning. This can be seen in \cref{fig:dr_fast_study} where learning could be halted when the eigenvalues have converged. However, the convergence depends on the initial learning rate as well as the parameter $p$ and therefore requires hand-tuning to avoid unnecessary long training times or premature stopping. This can be seen again in \cref{fig:dr_fast_study}, in which for $p = 0.09$, the network is performing adequate much before the eigenvalues or the largest deviations would suggest good results can be obtained. Only after $\approx 4000$ epochs the largest deviation appears to level whereas the second non zero eigenvalue still does not appear to have converged yet. The reason the largest deviations do not show appropriate results can be explained when looking at the parameters itself.\\
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../../plots/Learning/Wf_dr_study.pdf}
	\caption{Learning of toy example \cref{eq:toy_example} with different learning rate drops $p$ starting from $\alpha^f_0 = 0.005$. Top panel shows the learning rate for each epoch. Middle panel the relative deviation and bottom panel the absolute error.}
	\label{fig:dr_fast_study}
\end{figure}
For the above example with $p = 0.09$, the weights of $\bmu{W}^f$ are shown at different times of the learning phase in \cref{fig:Wf_slices} rows 1 \& 2. The desired outcome after learning is that every parameter is close to zero. With \cref{fig:Wf_slices} it becomes apparent that certain columns of $\bmu{W}^f$ become trained early. The in late stages of training emerging diagonal lines are caused by the optimal value of these matrix parameters being close to zero and therefore making the relative deviation more visible for errors while the whole matrix is converging to the optimum.\\
The reason behind the surprisingly low error in the early learning stages lies in the choice of our input sequence $\bmu{c}$. The choice of $\bmu{c}$ in \cref{fig:reference_sol} only causes the neurons with quickly learned weights to spike and therefore giving a lower error then expected. The spiking neurons and therefore the error do not yield a complete representation of the training state.\\
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../../plots/Learning/Wf_slices.pdf}
	\caption{Relative deviation for matrix weights at different times of learning. Note the different colour scale. The top 2 rows display the learning of $\bmu{W}^f$ with $p = 0.09$. Bottom row for $p=0.005$. For training the same input sequences have been used.}
	\label{fig:Wf_slices}
\end{figure}
If we instead use a more complex external input sequence $\bmu{c}(t)$ that ideally forces all neurons to spike we obtain a different picture seen in \cref{fig:complex_seq}. Note that this evaluation is on the same data used for \cref{fig:dr_fast_study}. However with \cref{fig:complex_seq} the error follows both the eigenvalues as well as the largest relative deviation. It also becomes apparent that the lack of convergence seen in \cref{fig:Wf_slices} contribute two orders of magnitude of the error compared to the previous case. Moreover judging by the error plots the choice of $p = 0.005$ vastly outperforms the previous pick of $p= 0.09$. With the lower drop rate $p$ imbalances that dominate the complete learning like in \cref{fig:Wf_slices} can be corrected faster as the learning rate has not dropped significantly yet, seen by the row 3 of \cref{fig:Wf_slices}.\\
Lastly, the choice of learning input sequences can be questioned to avoid the imbalances in early stage learning. So far for all previous examples input sequences were smoothed Gaussian white noise. The role of the input sequence will be considered when both matrices are trained.\\
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../../plots/Learning/Wf_complex_c.pdf}
	\caption{Evaluation of training progress using a oscillating input $\bmu{c}(t) = [c_1(t),c_1'(t)]^T$ with $c_1(t) = (1-e^{-4t})\sin(3\pi t)$ with different drop rates $p$. Top panel shows the non-zero eigenvalues. The middle panel the largest relative deviation and the bottom panel the maximum error during simulation in log-log scale.}
	\label{fig:complex_seq}
\end{figure}



\todo{Ein beispiel mit groesserem system und eins mit schlechten bedingungen.}

All in all, good approximations of $\bmu{W}^f$ can be learning with the given learning rule, yet the hyper-parameters $p,\alpha^f$ as well as learning period are sensitive to the problem at hand and therefore require hand tuning. Moreover, the shown learning results were obtained with ideal learning conditions.\\
Learning networks with more neurons or random initialization of $\bmu{W}^f$ complicates the learning phase however good learning results can still be achieved, be it with an increase of learning time.\\
When accessing the state of training it is crucial to choose appropriate test sequences to reflect different sources of error. This means to force all neurons to spike as well as cutting external input such that the network evolves by itself. Former examines of matrix weights have been learned for all neurons while the latter highlights small errors over time that do not become apparent when external input is dominating. The small errors incurred from suboptimal resets accumulate over time if no external input is present and $\bmu{\Gamma} $, $\bmu{W}^s$ are chosen optimally. Therefore a combination of both presented sequences should be used.\\
Lastly, our measures to access the network's learning progress rely on the knowledge of the optimal matrices or the repeated expensive computation of eigenvalues. Without online measures frequent testing is a necessity to avoid extensive learning periods.\\
\subsubsection{Spiking behaviour}
So far, we have not touched on the spiking behaviour. For the previous examples, the learning rule of \cref{eq:Wf_learning_rule_1}. Said learning rule does not incorporate $L_2$ costs in encourage distributed spiking.
When looking at raster plot in the middle panel of \cref{fig:Wf_learned_spikes} this disadvantage becomes apparent. Spiking is shared among 4 neurons at most.
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../../plots/Learning/biologic_plausibility_addition_mu.pdf}
	\caption{Top panel: Relative Spike Difference between the network with $L_2$ costs added to $\bmu{W}^f$ after learning and no addition. Error is given in multiples of decoder norm $\|\bmu{\Gamma}_i\|_2$. Middle panel: Raster spiking plot of $N=50$ neurons after simulation without the $L_2$ costs added afterwards. Bottom panel: Raster plot with costs added. Both matrices were trained for 1000 epochs with identical settings and received a scaled sin wave as external stimulus $\bmu{c}$. For these simulations, noise terms during training and simulation have been disabled.}
	\label{fig:Wf_learned_spikes}
\end{figure}
This is biologically implausible. When using the adapted learning rule of \cref{eq:Wf_learning_rule_improved} other problems arise.\\
As noted in the supplementary of \cite{brendel_learning_2020}, this particular learning rule does not learn the optimal weights but the optimal structure. As such it is therefore only a qualitative learning of the $\bmu{W}^f$. The scaling factor is now known a priori. The different scale of $\bmu{W}^f$ is detrimental to the networks convergence. In test no parameter set was found that mitigated this.\\
It is therefore mandatory to allow the Feed-Forward $\bmu{\Gamma}$ weights to adjust i.e they are learned as well. With Feed-Forwards also scaling over the course of training allows the different scales to balance out.\\
Learning of $\bmu{\Gamma}$ was adopted from \cite{brendel_learning_2020} which allowed the network to converge.\\
After convergence was achieved, the hyper-parameter scaling factors for $\bmu{W}^f$ and $\bmu{\Gamma}$ were tuned.\\
Goal of the tuning was to empirically find a parameter set of scaling factors s.t. the scale of $\bmu{\Gamma}$ remains unchanged throughout learning and transfer this to the learning with fixed $\bmu{\Gamma}$. However this again led to unusably erroneous results or failure in convergence outright.\\
The error itself in \cite{brendel_learning_2020} is not calculated using the learned decoder $\bmu{\Gamma}$ (noted as $\bmu{F}$) but the mathematically optimal decoder
\begin{equation}
\bmu{F}^T = \min_{\bmu{F}^T} \|\hat{\bmu{x}} - \bmu{x}\| = \min_{\bmu{F}^T} \|\hat{\bmu{x}} - \bmu{F}^T\bmu{r}\|
\end{equation}
using conventional methods of regression. While the learned decoding weights do converge, they lack behind the presented precision my orders or magnitude.\\
It is important to note that all the above testing was conducted only on the original example with $A = -\lambda_d\bmu{I} \longrightarrow \bmu{W}^s = \bmu{0}$. With custom $\bmu{A}$ convergence was not achieve with any parameter set or configuration.\\
This lack of convergence, the sensitive hand-tuning of the scaling parameters for $\bmu{W}^f$ and $\bmu{\Gamma}$ already without the training of $\bmu{W}^s$, the adoption of the improved learning rule \cref{eq:Wf_learning_rule_improved} was rejected.\\
Instead, to reestablish a more plausible spiking behaviour we
artificially introduce the diagonal term from the analytic solution for $\bmu{W}^f$. Since the first learning rule learns is approaching the $\bmu{W}^f = \bmu{\Gamma}^T\bmu{\Gamma}$ the addition is the most convenient way to reach our target ideal matrix $\bmu{W}^f = \bmu{\Gamma}^T\bmu{\Gamma} + \mu\mathbf{I}$.While this addition is biologically implausible itself\todo[inline]{Is it really? Could it be maybe somehow plausible?} we get more plausible spiking behaviour with little to no performance impact in return. The change in spiking behaviour can be seen in \cref{fig:Wf_learned_spikes} bottom panel.\\
 In comparison to the ideal network, both simulations performed well with an absolute error less than 0.1 compared the optimum. The dynamics between both networks also differ only marginally. In the top panel the differences in networks dynamics are displayed by the relative difference of spikes. For each spike of neuron $i$, the output signal shifts by $\bmu{\Gamma}_i$ as it is added to the spike train. From the top panel of \cref{fig:Wf_learned_spikes} it can be seen that the differences between both simulations were never more then 3 spikes different from each other.\todo[inline]{Is this saying smth? Maybe the absolute error is better?}

\todo{Eigenvalues are again interesting because with smaller eigenvalues we can use  that when the eigenvalues are not changing anymore the approx is ok-good.}

\subsection{Only learning of $\bmu{W}^s$}
\todo{Skip this since in the origninal paper it is already stated that the slow learning rule only works properly with the fast learning in tandem}
\subsection{Combined Learning}
For this chapter we consider the combined learning performance of slow weights $\bmu{W}^f$ and slow weights $\bmu{W}^s$. We skip the investigation of $\bmu{W}^s$ alone as the authors of the slow emphasize to learn both matrices simultaneously. Indeed, slow weights struggle to converge without learning of $\bmu{W}^f$.\\
For the investigation of the training of $\bmu{W}^s$ we apply the previously mentioned improvements of $\bmu{W}^f$ too. We start by looking again at our simple system before investigating parameter and input dependencies.\\
To begin, we look again at results for \cref{eq:toy_example} for reference. We adjust our external input sequence $\bmu{c}(t)$ to excite each neuron to spike so we have a better picture of whole learning progress of all neurons. Yet we keep a segment in which no input is given such than we can see the network's own evolution over time.\\
In \cref{fig:Ws_intro} the ideal results are displayed again with the given external $\bmu{c}(t)$ input.\\
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../../plots/Learning/Ws_simple.pdf}
	\caption{Exemplary results for our network with ideal weights. In the top panel the external input $\bmu{c}(t) = [\bmu{0},c]^T$. Bottom panel shows the calculated system response using numerical methods and the networks response. The network superimposes the numerical results exactly.}
	\label{fig:Ws_intro}
\end{figure}

To investigate the learning we will again look at the the measures defined in the previous section. Moreover, we will measure which matrix is more detrimental to the overall performance and therefore should be learned more accurately. Lastly we will conduct a parameter analyses so identify ideal settings. Lastly we will investigate how the training sequences influence the learning behaviour. So far, all learned was done with smoothed Gaussian noise. But so far very little attention went into our choice of learning input.\\
Starting we first compare the trajectories of the ideal against the learned weights to see where potential discrepancies arise. In \cref{fig:Ws_ideal_learned_compare} we can observe the networks response for learned and ideal weights after training are identical. Moreover the error is significantly better than the training of just $\bmu{W}^f$.
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../../plots/Learning/error_compare.pdf}
	\caption{Computed error between ideal and learned weights. Bottom panel shows the error over time between $x_1$ and $x_2$. Matrices were trained for 4000 epochs.}
	\label{fig:Ws_ideal_learned_compare}
\end{figure}
While the error is generally acceptable, it becomes clear that the error sources change when the external input $\bmu{c}(t)$ is absent and the network evolves by its own. While in the beginning, the network is driven by the external input, neurons fire frequently. Therefore, resets after spike are given by $\bmu{W}^f$ play a heightened role. Slight errors can cause a delay or pre-firing of a spike compared to the ideal weights. This can be seen in \cref{fig:Ws_ideal_learned_compare} just before $0.1$s, where the error jumps up after the learned network fired spikes the ideal network. The error jumps back down after the ideal network catches up. However later this discrepancy becomes too large and the error appears somewhat chaotic.\\
With the absence of the external input the error becomes fine grained again since the repeated spiking of caused by external input disapsdasdasdapears. Now the weights $\bmu{W}^s$ play an important role. Errors in the slow connections can add up slowly and cause wrong neurons to fire.\\

\subsubsection{Parameter analyses}
The following subsections are dedicated to investigate the parameters concerning the learning of $\bmu{W}^s$ and the learning of the network in general. This includes the error weight $K$, the slow learning rate $\alpha^s$, the learning input sequences to train the network.
\paragraph{Learning rate}
Looking at \cref{fig:combined_learning_lr} the learning rate it appears much larger learning rates can be used for $\bmu{W}^s$. While all tested learning rates $\alpha^s$ reach the same level of error, the larger $\alpha^s$ the faster this level is reached. Furthermore it becomes clear that the learning of $\bmu{W}^s$ is related to the accuracy of $\bmu{W}^f$. Only after deviations in the parameters of $\bmu{W}^f$ are small the learning of $\bmu{W}^s$ can reach errors less than 0.1 which corresponds to a relative error of
$\approx 10\%$. It is also clear that the learning rate of $\bmu{W}^s$ does not affect the behaviour of $\bmu{W}^f$ and therefore can be learned with the same or larger learning rates. This is in opposition to \cite{bourdoukan_enforcing_2015} where is is recommended to learn $\bmu{W}^s$ with a smaller learning rate compared to $\bmu{W}^f$.\\
We choose $\alpha = 0.01$ for future evaluations as it marginally improves performance in the error and shows less variability in the rel. deviations. This comes at the cost of training for 100 more epochs before reaching slightly better performance.\\
To combat the unsteady behaviour in the first panel we again try again to reduce the learning rate over time.\\
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../../plots/Learning/combined_learning_lr.pdf}
	\caption{Top panel showing the largest rel. deviation after each epoch for different learning rates $\alpha^s$. Middle panel shows the maximum error during simulation with the learned $\bmu{W}^f$ and $\bmu{W}^s$ in log scale. Bottom panel shows largest rel. deviation for $\bmu{W}^f$.}
	\label{fig:combined_learning_lr}
\end{figure}

To do this we again set our learning rate according to \cref{eq:drop} and test different $p$. In \cref{fig:combined_learning_dr} we see the effect of different values for $p$. As expected was the error in the bottom panel of \cref{fig:combined_learning_dr} not reduced by this addition. Moreover, under a linear scale of the largest rel. deviations are not noticeable and will therefore be omitted for future tests. Lastly, we did not look at the eigenvalues of $\bmu{W}^s$ yet. Fortunately, the non-zero eigenvalues do allow for a coarse estimate of the learning progress. In all test including \cref{fig:combined_learning_lr} the eigenvalues converged to the optimal values and usable results could be obtained after both eigenvalues converged.\\
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../../plots/Learning/combined_learning_drop.pdf}
	\caption{Top panel showing the two non-zero eigenvalues  of $\bmu{W}^s$ after each epoch for different drop rates $p$ and starting learning rate of $\alpha^s = 0.1$ . Middle panel shows largest rel. deviation for $\bmu{W}^s$. Bottom panel shows the maximum error during simulation with the learned $\bmu{W}^f$ and $\bmu{W}^s$.}
\label{fig:combined_learning_dr}
\end{figure}


\paragraph{K}
The parameter $K$ describes the influence of the error during training. With increasing $K$ the importance of the error and therefore the teacher grows.\\
As can be seen in \cref{fig:combined_learning_K}, the parameter $K$ only slightly impacts the network's learning. Also in contrast to the derivation in which $K$ is assumed to be large, the smaller the K the better the network performs. Furthermore, for this example values $K>100$ did not give acceptable results. For $K>500$ convergence did not in the $4000$ epochs of learning.\\
In the middle panel of \cref{fig:combined_learning_K} tested values above $50$ ($K = 100, 200,500,1000$ not all shown) present a bend in the error after $\approx 1050$ epochs and the error starts increasing again. With larger $K$ the bend is more pronounced and occurs after fewer epochs.\todo[inline]{Reason?}
On the other hand values $5>K>0.5$ all performed almost identical to $K=1$ shown in the plot.\\
In the last panel of \cref{fig:combined_learning_K} we chose $K=10$ to test the main source of error. For this we ran the simulation with $\bmu{W}^f$ learned using the optimal $\bmu{W}^s$ and vice versa. The results are seen in the bottom panel of \cref{fig:combined_learning_K}. Without a good approximation of $\bmu{W}^f$ the error is more than 2 orders of magnitude worse than the opposite configuration. However this normalizes at around $30$ epochs after which the the error in $\bmu{W}^s$ dominates. Only after more than $1000$ epochs $\bmu{W}^s$ is accurate enough such that the small errors in $\bmu{W}^f$ from learning become relevant once more. However after more than $1000$ epochs the error less than $0.04$ or $\approx 3\%$ relative error.\\
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../../plots/Learning/combined_learning_K.pdf}
	\caption{Top panel shows the rel. deviation for each $K$. Middle panel shows the maximum error during simulation after each epoch using both $\bmu{W}^f$ and $\bmu{W}^s$ from training using different $K$. Bottom panel shows the error for $K = 10$ while using either the optimal $\bmu{W}^f$ or $\bmu{W}^s$ respectively.}
	\label{fig:combined_learning_K}
\end{figure}

\subsubsection{Input learning sequences}
So far we have not touched on the training input. As often seen in the literature\todo[inline]{put some references here}, training is performed using smoothed Gaussian noise. However little detail is given on the specifics. Moreover, as speculated above, the choice of input sequence could potentially affect the learning results.
Therefore, in the next section we will investigate different patterns of input and potential influence on the results.\\
In the right training sequence we may find ways to make our network even more accurate.\\
Firstly we note the general framing of our learning. So far we have used the common smooth Gaussian with an exponential kernel. Already the number of tunable parameters is large when considering different kernels to get smoother or more variable input sequences. However there are also completely different functions that can be tested. For now we compare the currently used Gaussian noise again pure sine waves and individually stimulating each neuron for training.\\

\paragraph{Gaussian noise and smoothing}
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../../plots/Learning/combined_learning_smoothness.pdf}
	\caption{Illustration of smoothing with $\sigma$ changing in the kernel. F.l.t.r $\sigma = \begin{Bmatrix}
		6 & 60 & 600\end{Bmatrix}$. Plots show only the first 10\% of a learning epoch for visibility.}
	\label{fig:generate_input_seq}
\end{figure}
\todo[inline]{integrate the above figure in the text!}
The current setup of learning is based on a sequence $\bmu{v}$ of Gaussian noise that is generated for each epoch. One sequence is 100000 timesteps long and each gets convoluted by a exponential kernel to obtain $\bmu{c}$. The standard deviation for $w$ was chosen tight to allow for rapid movement of $\bmu{c}$ allowing for great variety which drives the error and subsequently the learning of $\bmu{W}^s$.\\
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../../plots/Learning/combined_learning_smoothness.pdf}
	\caption{Learning results for different smoothing grades of $\bmu{c}$. Smoothing is governed by the std. dev. $\sigma$ of the exponential kernel. Top and bottom panel shows the largest rel. dev. of $\bmu{W}^f$ and $\bmu{W}^s$ respectively. Middle panel shows the absolute error. Trained was for 1000 epochs.}
	\label{fig:combined_learning_smoothness}
\end{figure}
This can be seen in \cref{fig:combined_learning_smoothness}. For $\sigma = 6$, a tiny smoothing window, the networks shows the best results with an maximum absolute error of $0.03$ which is within one spike difference from the ideal with the given setup. The second best option is for $\sigma = 60$ with double the error of $\sigma = 6$. Also in the convergence of both $\bmu{W}^f$ and $\bmu{W}^s$ the least amount of smoothing appears to yield the best results. Though this is heavily depending on the signal that the network is being tested with.
Moreover difference in error is reduced when tested with different signals $\bmu{c}$ to input. With oscillating input similar to what was used in training the network or larger amplitude $c$, the trials with more smoothing reduce the difference in the error almost completely.\\
\todo{Figure for illustrating the different smoothing.}
\paragraph{Alternative Learning inputs forms}
The importance of sudden movement and therefore the necessity for tiny smoothing becomes apparent when comparatively smooth input sequences are tested. To illustrate this the network is trained with the previous white noise with $\sigma = 60$ and compared to similar magnitude sine and square waves.\\
To introduce some variability in the sine wave. For each epoch the phase for each input is varied.\\
The square wave is explicitly structured to enforce firing of 2 specific neurons at a time. As illustrated before, each neuron $i$ projects the error along $\bmu{\Gamma}_i$ direction and fires when the threshold is reached. To target neuron $i$ directly we project the square wave on each dimension $c_j$ of the input by multiplying it with $\bmu{\Gamma}_{ij}$. Using this approach neuron $i$ fires the earliest and will be trained. During the off cycle of the square wave the same happens with the neuron geometrically opposing $i$.\\
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../../plots/Learning/spike_learning_input.pdf}
	\caption{Cumulative spikes for each neuron over training. Top: Gaussian noise, Middle: Sine wave, Bottom: Square wave. Only a segment of the entire training process is shown. Note the different colour scales.}
	\label{fig:spike_learning_input}
\end{figure}
To verify this we can look at the spiking behaviour during training. In \cref{fig:spike_learning_input} we can see the cumulative spiking of each neuron during training for the Gaussian noise, sine and square wave. The square wave clearly shows our desired pattern in which for each epoch a new pair of neurons is trained, illustrated by the diagonal edges. From this it can also be seen that the test used 50 neurons. At the mark of 50 epochs a rotation is completed indicated by the first diagonal line ending there. Furthermore after 25 epochs each pair of neurons has been trained once, indicated by the 2 diagonal line starting from the top left corner entering the section of geometrically opposing neurons.\\
Surprisingly the spiking behaviour with the sine wave and the Gaussian noise exhibit a aggregation of spikes only on certain neurons. From the colour grading it becomes apparent that certain neurons spike more often during training than others. While this is caused by the lack of regularization and subsequent distribution to spikes in our learning scheme due to aforementioned problems. Solely from judging \cref{fig:spike_learning_input} it would be natural to guess that the learning using square waves would perform the best thanks to the even distribution of spikes and therefore even learning. However this would be wrong as can be seen in \cref{fig:sin_square_bad}. While the trajectory for $x_2$ follows the main numerically calculated solution in blue, $x_1$ does not follow the expected trajectory at all. This misbehaviour is caused by the input sequences overshooting the scale of the parameters of $\bmu{W}^s$. Similarly this occurred as well when using heavily smoothed noise. The sine and square wave created, similar to the heavily smoothed, stretches of repetitive stimulus appears to cause the corresponding weights for spiking neurons to be oversaturated. Lowering the input amplitude did improve performance at the cost of longer training times. With the reduced input the convergence of $\bmu{W}^f$ is slowed down by the reduced number of spikes necessary. Furthermore $\bmu{W}^s$ learning rule is directly depending on the error and the rate, therefore depending on the state vector $\bmu{x}$ that is once more dependent on the input $\bmu{c}$.
\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../../plots/Learning/sin_square_bad.pdf}
	\caption{Network responses for each training sequence type with a oscillating input $\bmu{c}$.}
	\label{fig:sin_square_bad}
\end{figure}

\paragraph{Spike firing}

das ungleiche feuern abhanging von A
die input sequenzen
das manche neuronen die anderen das feuern stehlen
warum die anderen sequenzen scheisse sind (fuer Ws)
Anpassen der Lernsequenz so dass alle neuronen gleich oft feuern
Finde die anzahl an spikes die man braucht um ausreichend gelernt zu haben

In training, each neuron is fired ideally the same number of times to facilitate evenly distributed training progress. However, when looking at the spike numbers during training this is not the case. In fact, the learned system $\bmu{A}$ is crucial for the spike distribution. This can be seen in \cref{fig:spike_dist}, where the cumulative sum of spikes are added for each epoch on 2 different systems $\bmu{A}$. The crucial difference is that our model problem's eigenvalues are further apart. Generally it was observed that the distance between eigenvalues relative to their absolute values was the determining factor.\\
The reason of lies in $\bmu{W}^s$. If $\bmu{A}$ exhibits a large difference in its eigenvalues this is reflected in $\bmu{W}^s$. During training, neuron voltages increase more for neurons projecting in the dimension of the large eigenvalue, causing it to spike. This can be seen again in \cref{fig:spike_dist}, where the neurons with the highest spike count

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{../../plots/Learning/combined_learning_smoothness.pdf}
	\caption{Spike distribution for our model problem and $A = -10\bmu{I}$. Cumulative sum of all spikes for each epoch.}
	\label{fig:spike_dist}
\end{figure}
\todo{using less smooth input helps distribute spikes better. Therefore more distributed learning etc.. better }

\todo{Reason? Eigenvalues? or Separation between eigenvalues? Off diagonals in the eigenvalues?}

\subsubsection{Random init of matrices}
As we have done in the section \cref{ssec:Wf_res}, results so far have only considered optimal conditions for learning. In light of more challenging conditions, especially the initialization of $\bmu{W}^$ and $\bmu{W}^s$ to zero is a significant aide towards the learning and network performance.\\
Firstly, the choice of $\bmu{\Gamma}$ did not influence the results significantly during testing and will therefore be ignored in this comparison. To test the influence of initial weights in both matrices we initialize the matrices with random numbers scaled to the appropriate range of $\bmu{W}^f$ and $\bmu{W}^f$ respectively.\\
For the first test both matrices were initialized to random number of the same scale as their optimal values. Results showed that from 1000 epochs of training 920 networks did not converge at all while in the remaining 80 epochs results were unusable. It was observed that while training the matrix eigenvalues diverged further from their optimal counterparts. This was also observed when measuring the largest rel. deviation.\\
When initializing with random values 50\% of the scale convergence was obtained during all of training. However the relative error was more than 100\% therefore rendering results useless.\\
If only either $\bmu{W}^f$ or $\bmu{W}^s$ is initialized randomly it becomes apparent that the $\bmu{W}^s$ is the reason for the aforementioned problems. When initializing $\bmu{W}^s$ to zero, acceptable results were obtained with $\bmu{W}^f$ initialized to the previously impossible 100\% noise scale. However this was only achieved with at least doubling the training time.\\
On the contrary, the initialization of $\bmu{W}^s$ just 5\% noise scale prohibited any convergence during training. The hypothesis, that potentially flipped signs in $\bmu{W}^s$ being the reason for the lack of convergence was rejected as there was no change when each $\bmu{W}^s_{ij}$ was given the same sign of the respective analytical value.\\
Thus, $\bmu{W}^s$ has massive influence on $\bmu{W}^f$. This can also be seen when attempting to run a simulation with learned $\bmu{W}^s$ and analytical $\bmu{W}^f$ as the network fails to converge. Yet when simulating the opposite configuration results can be obtained, although of subpar quality to the completely learned configuration.\\
This shows that $\bmu{W}^s$ influences $\bmu{W}^f$ to adapt to the (wrong) structure of itself. Hence it is advisable to only allow noise to impact $\bmu{W}^f$. While this notion deviates from biologic plausibility since noise is inherent to any system and connectivities between neurons are not nullified at when learning commences, this is an manageable restriction for the sake of usable results.

\subsection{Limitations}
Limitations in the learning of $\bmu{W}^s$ arise when the eigenvalues of $\bmu{A}$ have positive real parts. For purely imaginary eigenvalues, results depend on the magnitude of the imaginary part. For increasing imaginary parts the matrices converge to the correct structure be it not the correct magnitude. This can be combatted by tuning of hyperparameters though it depends on the $\bmu{A}$, making it infeasible for wide range use. Without tuning, the error remains high and results unusable.\\

\todo[inline]{Elaborate. What are other limitations?}

& Teste das mit den spikes noch ein wenig. Dann nur noch die controller results und limits.
Conclusion ist dass es nicht geeignet ist fuer ieine anwendung aber es schon ein system regeln kann.
&Dann in final words sagen was wir gemacht haben und iwie auf das intro gehen
& Fixes und rewrites im Text!

Long simulation times\\
High order systems ?? Testen..
Variability in learning performances
\section{Results of the learned control objective}
Adjust connectivity to have "fake" 2 states in the neurons?
%Ich meine dass mann in einem 2d beispiel 50 neuronen nur fuer state x1 und 50 neuronen nur fuer x2 und das dann lernen und dann kann man argumentieren dass nur gewisse neuronen angeregt werden koennen (durch die selective von F) und damit sozusagen eine B matrix eingefuert wurde.

Einmal performance mit 0 init
Verschiedene Referenz trajectories
Limits
Das Ding mit dem B

Limits