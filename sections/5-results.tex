\chapter{Results}
Describe the results of the degree project.
Analyses of results

How does the optimal network simulate?
Influence of parameters\\
What are the magic parameters? mu etc



The control with optimal works fine too.
Influence of the parameters
Magic number tuning required for x,y,z etc

Learning works fine with magic numbers
Examples.
What works better?
What works worse?
What if I only learn 1 part? What is the bigger error.
Convergence over time usually very bad
Compared to the eigenvalues maybe convergence improves
How do the parameters look? Heatmap
How to judge the accuracy of the closeness of the parameters to the optimal
What is the influence of the input
You have to juggle the values of learning rate and input amplitude to reach ideal results



\section{Results on the Simulation from \cref{sec:res_simulation}}
\subsection{Toy Example}
\begin{figure}[h!]
	\centering
	\centering
	\includegraphics[width=\textwidth]{../../plots/Simulation/basic_example.pdf}
	\caption{Baseline example}
	\label{fig:sim_res_1}
\end{figure}

The results from simulating the network with given input $\bmu{c}(t)$ can be seen in \cref{fig:sim_res_1}. In this scalar example the ODE
\begin{equation}\label{eq:leaky_integration_example}
\dot{x} = -10x +c(t)
\end{equation}
is simulated with 2 neurons. One neuron corrects the network simulation of the system up and down respectively. As shown before this correction happens immediately after each spike by adding weights of the relevant decoding vector to the trajectory. Since this example is following a scalar variable with 2 neurons the decoding matrix $\bmu{\Gamma}\in \mathbb{R}^{1\times2}$ and set to $\left[0.09,-0.09\right]$ for this example. This can be seen in \cref{fig:sim_res_1}, the neural network simulation jumps up after each spike by 0.09 is added. The external input follows a linear increase until 0.15s after which it remains constant.\\
For reference a conventional numeric solution is given which lies directly between the two neurons threshold, visualized with dotted lines.\\
\begin{figure}[h!]
	\centering
	\begin{subfigure}[t]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../../plots/Simulation/small_lambdaD.pdf}
		\caption{Reduced Readout Decay rate}
		\label{fig:sim_low_lambda}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../../plots/Simulation/small_gamma.pdf}
		\caption{Reduced Threshold}
		\label{fig:sim_low_gamma}
	\end{subfigure}
	\caption{Variation of Readout Decay and Decoder for simple 1D system.}
	\label{fig:sim_res_2}
\end{figure}
Already for this toy example different parameters can be tuned to get different results. In \cref{fig:sim_low_lambda} the readout decay is reduced compared to \cref{fig:sim_res_1} which reduces how fast the output tends to zero. This also elevates the importance of a single spike as it has longer lasting effects on the output, seen by the system showing fewer spikes than before.\\
Alternatively, if the decoding weights can be scaled to let each spike make a smaller change in the output seen in \cref{fig:sim_low_gamma}. Since the threshold is closely tied to the Decoding weights this also reduces the spike threshold and therefore yields more accurate results.\\

\subsection{Toy example in 2D}

\begin{figure}[h!]
	\centering
	\includegraphics[width = \textwidth]{../../plots/Simulation/simple_2d.pdf}
	\caption{Simple 2D example with numerical solution and spike response. Curves for $x$ in yellow/blue. Curves for $y$ in purple/red. The networks output closely tracks the perfect numerical solution. For the network each decoding vector was chosen from a normal distribution and normalized to $\left\lVert\bmu{\Gamma}_i\right\rVert_2 =0.3$. Beneath a raster plot for each spiking neuron \\ On the left the threshold for each neuron's projected error.}
	\label{fig:sim_res_simple}
\end{figure}



\subsection{Geometry in 2D}
\begin{figure}[h!]
	\centering
	\includegraphics[width = \textwidth]{../../plots/Simulation/2d_simple_spikes.pdf}
	\caption{Simple 2D example with numerical solution and spike response. Curves for $x$ in yellow/blue. Curves for $y$ in purple/red. The networks output closely tracks the perfect numerical solution. For the network each decoding vector was chosen from a normal distribution and normalized to $\left\lVert\bmu{\Gamma}_i\right\rVert_2 =0.3$. Beneath a raster plot for each spiking neuron \\ On the left the threshold for each neuron's projected error.}
	\label{fig:sim_res_geometric}
\end{figure}
In two dimensions the network with its allows for a geometric interpretation. For this we let the network simulate a simple leaky integration of inputs as in \cref{eq:leaky_integration_example} but in two dimensions. We have the corresponding results in \cref{fig:sim_res_geometric} on the right. On the left we a phase plot in the $xy$-axis.


\subsection{Importance of Feedforward/Decoding weights}
So far we have not given much attention to decoding/feedforward weights. Yet they play a crucial rule in the performance of our network as seen in \cref{fig:sim_res_3}. Here we simulate again a simple leaky integration as in \cref{eq:leaky_integration_example} however this time in 2D. With the output, in each test we also plot a bounding box for the relative error.\\
The bounding box of the error is the normal to each of the decoding weights. In \cref{eq:x_hat} the network output gets the $i$th column $\bmu{\Gamma}$ added when the $i$th neuron spikes. From the minimization of the cost we know that the spike of neuron $i$ reduces the error in projected by $\bmu{\Gamma}_i$. If we know imagine the origin of the \cref{fig:sim_res_3} l to always be on top of the true trajectory of $\left[x ,\ y\right]^T$, the network's error is just deviation from the origin.

Explain phase plot
explain the normal to the decoding weights
Explain the 3 plots
Dont forget to explain the Trichter in the bad plots
\begin{figure}[h!]
		\centering
	\begin{subfigure}[t]{0.6\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../../plots/Simulation/2d_perfect_gamma.pdf}
		\caption{Network }
		\label{fig:sim_perfect_gamma}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.6\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../../plots/Simulation/2d_random_gamma.pdf}
		\caption{Reduced Threshold}
		\label{fig:sim_random_gamma}
	\end{subfigure}
	\hfill
\begin{subfigure}[t]{0.6\textwidth}
	\centering
	\includegraphics[width=\textwidth]{../../plots/Simulation/2d_bad_gamma.pdf}
	\caption{Reduced Threshold}
	\label{fig:sim_bad_gamma}
\end{subfigure}
	\caption{Network output after leaky integration of oscillating input with different decoding vectors.}
	\label{fig:sim_res_3}
\end{figure}


\subsection{Bigger Systems}
\begin{figure}[h!]
	\centering
		\includegraphics[width=\textwidth]{../../plots/Simulation/big_system.pdf}
	\caption{Trajectory of dynamic mass spring system with 100 masses and 2 thousand neurons. Only the first 3 trajectories are plotted and the spikes for the first 200 neurons.	}
	\label{fig:big_systems}
\end{figure}

So far we have only looked simple systems in 1D or 2D. However our network can also simulate more complex higher dimensional systems.
In the example in \cref{fig:big_systems} we simulated a linear n-mass spring system with the dynamics
\begin{equation}
	m_i\ddot{x}_i = K\cdot \begin{bmatrix}
	1&-2&1
	\end{bmatrix} \cdot	\begin{bmatrix}
	x_{i-1}\\
	x_i\\
	x_{i+1}
	\end{bmatrix} + c_i(t).
\end{equation}
The external forces were each offset sinusoidal waves with varying frequency and amplitude for the first 3 masses and random forces for the rest. As can be seen from the figure, the network perfectly overlays the numerical solution.


\subsection{Varying cost parameters $\mu,\nu$}
\begin{figure}[h!]
	\centering
	\begin{subfigure}[t]{0.6\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../../plots/Simulation/2d_cost_none.pdf}
		\caption{Network }
		\label{fig:sim_no_cost}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.6\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../../plots/Simulation/2d_cost_nu.pdf}
		\caption{Reduced Threshold}
		\label{fig:sim_nu_cost}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.6\textwidth}
		\centering
		\includegraphics[width=\textwidth]{../../plots/Simulation/2d_cost_mu.pdf}
		\caption{Reduced Threshold}
		\label{fig:sim_mu_cost}
	\end{subfigure}
	\caption{Network output after leaky integration of oscillating input with different decoding vectors.}
	\label{fig:sim_res_4}
\end{figure}

\section{Results on the control}
For the evaluation of the results for the control problem derived in \cref{sec:control} we first note a few important implementational details in order to get useful results. \\
Afterwards we compare the performance of the two different derivations from \cref{ssec:control_dynamics,ssec:extension} for specific examples as well as their limits.\todo{For example dirac doesnt work,Actually anything with a discontinuity doesnt work???,long simulation times. All for the one with  error.}
Lastly we take a closer look at the main reason for this system to work and how we can adopt it to our learning problem in \cref{sec:learning}.

\subsection{Implementation details}
Before any simulation can be run it is important that certain detail in the implementation are set in the correct order for the results to make sense.


\subsubsection{Multiple spikes per timestep}
Although this also applies for the simulation step in \cref{sec:res_simulation}, its affects are much more pronounced in the control setting.\\
\paragraph{Single Spike}
As the dynamics outlined suggest the simulation step includes a check if a threshold has been reached. \\
A simple implementation in MatLab pseudo code is seen in \cref{lst:single_spike}.

\begin{lstlisting}[language=Matlab, caption=Single spike implementation,label=lst:single_spike]

for t_step = 1:N_step
	V = update_Voltages(V,dt,Ws,Wf,c,...);
	[value, index] = max(V-Threshold);
	spikes(index,t_step) = 1;
	V = V + Wf(:,k);
	...
end
\end{lstlisting}
While this works for a variety of problems it does not give results for any given system and reference trajectory.\\
Especially problems with an abrupt change underperform since the network is only allowed to correct the error by one spike. However by jumps and rapid changes this is not sufficient to compensate errors and then networks struggles for many iterations to recover, ruining the overall accuracy in the process.\\
\paragraph{Parallel Spikes}
Now the direct way to fix this to find all neurons that have reached their thresholds. Since networks have many neurons, letting every neuron spike increases the networks ability to correct errors. A potential implementation is seen in \cref{lst:multi_spike}.
\begin{lstlisting}[language=Matlab, caption=Letting every neuron spike in parallel,label=lst:multi_spike]

for t_step = 1:N_step
	V = update_Voltages(V,dt,Ws,Wf,c,...);
	spiking_neurons = V>Threshold; % procudes a list
	spikes(spiking_neurons,t_step) = 1;
	V = V + Wf(:,spiking_neurons);
	...
end
\end{lstlisting}
The problem with this implementation is that the error gets tracked by multiple neurons simultaneously and also in both in positive and negative direction. To illustrate this we consider a simple example where we have a single control variable $u$ and $2N$ neurons, where $N$ neurons track positive and negative error respectively.\\
Disregarding noise, the threshold is reached by $N$ neurons synchronously. As long as the total error is larger than the compensation of $N$ neurons in the first place, this works just fine. However this is not permanent and at some point all $N$ neurons firing overly depolarize the opposite side neurons to the extend that they all fire in the next iteration regardless of the real system error.\\
The root cause is that a single spike influences the whole network. In this extreme case 1 spike resets all other $N-1$ neurons that were spiking as well and therefore would not give any performance boost compared to the previous approach.\\
The network with this implementation behaves normally while a rapid change is present but will evolve into $N$ neurons spiking in alternating cadence respectively.\\
To remedy this it is necessary that one dimension of the error is only projected onto two neurons. While this can be achieved by carefully selecting the decoder it is not a generic approach. Moreover this reverses the idea of multiple neurons tracking the error in order to increase the networks performance and would result in zero change.\\
Therefore we need to let a single neuron spike multiple times.

\paragraph{Multiple Spikes}
The correct way to handle this problem is to allow more than one spike per timestep but compute each spike's change in the network separately. This way we can achieve the necessary performance without the problems of the previous approach. A implementation of such a regime is seen in \cref{lst:correct_spike}

\begin{lstlisting}[language=Matlab, caption=Letting each neurons spike as many times a necessary while computing each spike's influence sequentially.,label=lst:correct_spike]

for t_step = 1:N_step
	V = update_Voltages(V,dt,Ws,Wf,c,...);
	[value, index] = max(V-Threshold);
	while value > 0
		spikes(index,t_step) = 1;
		V = V + Wf(:,k);
		...
		[value, index] = max(V-Threshold);
	end
end
\end{lstlisting}

\subsubsection{Signed Error and Slow Connectivity}
\paragraph{Neglected Term}
First to note is that the derivation of $\bmu{W}^s$  in \cref{eq:Ws_wrong} is different from the definition it \cite{huang_optimizing_2017} in two ways. Firstly in the original derivation the rate is scaled by $\lambda_d$ i.e.
\begin{equation}
	\hat{r}(t) = \lambda_dr(t).
\end{equation}.
Substituting this in for the equations in \cref{ssec:control_dynamics} gives the exact same dynamics.\\
Secondly and more importantly in their derivation consider network in the limit for $N\rightarrow \infty$ neurons. Specifically the derivation arrives at
\begin{equation}
\begin{aligned}
\dot{\bmu{V}}(t)= & \bmu{\Omega}^T \bmu{B}^T\left(\dot{\hat{\bmu{x}}}(t)-\dot{\bmu{x}}(t)\right)-\mu \lambda_d \dot{\bmu{r}}(t) \\
= & \bmu{\Omega}^T \bmu{B}^T \bmu{A} \bmu{L} \bmu{V}(t) \\
& +\left(\mu \lambda_d \bmu{\Omega}^T \bmu{B}^T \bmu{A} \bmu{L}+\mu \lambda_d^2 \bmu{I}-\frac{1}{\lambda_d} \bmu{\Omega}^T \bmu{B}^T \bmu{B} \bmu{\Gamma}\right) \bmu{r}(t) \\
& -\left(\bmu{\Omega}^T \bmu{B}^T \bmu{B} \bmu{\Omega}+\mu \lambda_d^2 \bmu{I}\right) \bmu{o}(t) \\
& +\bmu{\Omega}^T \bmu{B}^T(-\bmu{A} \hat{\bmu{x}}(t)+\dot{\hat{\bmu{x}}}(t))
\end{aligned}
\end{equation}
where $$\bmu{L} = \left(\bmu{B\Omega\Omega}^T\bmu{B}^T\right)^{âˆ’1}\bmu{B\Omega}$$ is the pseudo-inverse of $\left(\bmu{B\Omega}\right)^T$. Our focus is now set on the terms in front of the rate. As \cite{huang_optimizing_2017} and the original derivation in \cite{boerlin_predictive_2013} argue the term $\mu\lambda_d^2$ vanishes in the limit and can therefore be neglected, yielding the above result for $\bmu{W}^s$ above in \cref{eq:Ws_wrong}.\\
However in the testing it was noticed that this derivation only works acceptable for a small range of values for $\lambda_d$, explicitly $\lambda_d \leq 20$. For values above the performance deteriorates rapidly if not a large number of neurons is considered.\\
After testing the relative importance of terms it was noted that the term $\mu\lambda_d^2$ does improve the performance with a smaller number of neurons significantly compared to the other terms and was therefore added in the results following below.\\
Furthermore the authors give later examples that show their use of the extra term.
This can be seen in all examples but especially in example figure 2. Conveniently the authors provided a picture of the their $\bmu{W}^s$ matrix which clearly shows a nonzero entry on the main diagonal. The example showcases the case $\bmu{\Gamma} = \bmu{0}$ which, given the definition of $\bmu{W}^s$ in \cref{eq:Ws_wrong}, yields $\bmu{W}^s = \bmu{0}$. Examining the colormap reveals that the values on the diagonal equal to 1 which is the same $\mu\lambda_d^2$ in the given example. The same can be validated for the other examples.\\
Running the same example results in a subpar performance shown in \cref{fig:bad_Ws}. To match the behaviour shown in the paper we needed to either involve the omitted term or increase the number of neurons to $N= 500$. Even though in \cref{fig:bad_Ws_compensated} it might look like the behaviour is similar it is important to point out that over time the network's performance deteriorates over time similar to \cref{fig:bad_Ws} and certainly cannot match the behaviour shown in \cite{huang_optimizing_2017}.
\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{../../plots/Control/controller_without_extra_term.pdf}
	\caption{Control performance of the same the example in figure 2 from \cite{huang_optimizing_2017} without the extra $\lambda_d\mu^2\bmu{I}$ term. Simulation with $N=100$ neurons.}
	\label{fig:bad_Ws}
\end{figure}


\begin{figure}[h!]
	\centering
	\includegraphics[width=\textwidth]{../../plots/Control/controller_without_extra_term_compensation.pdf}
	\caption{Control performance of the same the example in figure 2 from \cite{huang_optimizing_2017} without the extra $\mu\lambda_d^2\bmu{I}$ but $N=500$ neurons to compensate.Neuron are split 50:50 as before.}
	\label{fig:bad_Ws_compensated}
\end{figure}



Table
Mit extra term
ohne extra term
 Fehler in einem beispiel mit extra
 anzahl neuron die man hinzufuegen muss um ausgleich zu haben


\paragraph{Error sign}
The second important implementational detail is that concerns the error term in \cref{eq:direct_error_voltage} and the corresponding plots shown in \cite{huang_spiking_2019}. Using the derivation found therein it was not possible to reproduce the results. After investigating the problem was found to be in the sign of the error term. To illustrate this it is useful to consider their basic example of a scalar system
\begin{equation}
	\dot{x} = -10 x + u\\
\end{equation}
but only two neurons with weights
\begin{equation}
\begin{aligned}
\bmu{\Omega} &= c\cdot\left[-1 , 1\right] \quad c>0\\
\bmu{\Gamma} &= \left[0,0\right]
\end{aligned}
\end{equation}
and no additional cost terms.\\
This configuration allows for a straightforward allocation of functions to individual neurons. In the provided illustration, Neuron 2's voltage tracks the error whenever the network output $\hat{x}$ lags behind the reference value $x$. It becomes active if the deviation surpasses
\begin{equation}
	V_2 = 1c\left(x-\hat{x} \right) < \frac{c^2}{2}
\end{equation}
prompting a corrective response to increase the network output.
We now consider the error term with $e(t) = x-\hat{x}$
\begin{equation}
\begin{aligned}
	\bmu{\dot{V}}&= \bmu{\Omega B}^T \bmu{A}e(t) + \dots\\
	\bmu{\dot{V}}&=  -10c \cdot\begin{bmatrix}-1\\1\end{bmatrix}e(t).
\end{aligned}
\end{equation}
Now, let's examine a scenario where the network is falling short of the reference value, leading us to deduce that $e(t)>0$. Consequently, we can conclude that the error term is conflicting with the intended definition of our voltage by inadvertently increasing the voltage of Neuron 1 in the wrong direction while it is Neuron 2 that should increase its voltage.\\
One way to resolve this issue is to add an artificial minus sign the error. However this only brings changes the problem to the case when $\bmu{A} = 10$. The other solution is to interpret the phrasing of the original paper literally and assume $\bmu{A}$ as a literal "gain" which therefore is usually greater than zero and take $|A|$ for the computation, yielding proper results. How either of these additions could emerge out of the derivation is so far unclear.\todo{Double check the example and verify it. Additionally maybe ask arvind about the whole thing.}\\
Only with either of these additions it was possible to reproduce the results show in the paper \cite{huang_spiking_2019}.\\
However it is interesting to note that in different test scenarios it was found that the error term contributes only a small amount to the voltage in \cref{eq:direct_error_voltage} and therefore could be omitted in many cases directly.
This holds also true for the example here where the omitted error makes the network less accurate but still usable compared to the method introduced in the original work which diverges rapidly.\\

\paragraph{Output Feedback}
How can I understand the output feedback? ASK ARVIND
Useless Output feedback???

Adjustment for the Outputfeedback, even though it is useless

If you have output feedback you still need to give the whole state to the network
so even though you have output, same goes for the derivative. So it is just useful if you want to give some output but not you know the whole system and the state so you can control. Which kind of makes it a bit useless.\\
Because this formulation does not allow e to be the error between network and reference. It has to be error between states. So my controller is just a greedy P controller?


\subsection{Numerical treatment of spikes? Aka scale by 1/dt}
Do it for the control signal but also for the reference signal (when there are jumps in the derivative for example)
\subsection{Performance Comparison}
Test1: kind of step function 1d with a couple neurons
Test2: many dimensions random neuron decoders and some oscilating ref
Accuracy
\# spikes
Looks like they perform identical???


\subsection{Limits}
prob noise in the inputs?
noise in the network
B'C' = 0 bereits klar
if there is a limit of how many spikes per time interval the magnitude is bound
size?? how to find a large controllable system?
inexact derivatives?




\subsection{Direct error/Feed-Forward}
The most important insight of the approach in \cref{eq:feeback_c,eq:feedback_c2} is that the signal $\bmu{c}(t)$ is the principal source of control in the system. Moreover it is explicitly calculated from the system $\bmu{A}$ as well as the information of the reference signal and its derivative.\\
If one considers the output feedback once more and where one does not have access to the target state $\bmu{x}$ but only its output $\bmu{y} = \bmu{Cx}$, the derivation breaks down. Instead the feed-forward input $ \bmu{c}$ would need to be defined in terms of $\bmu{y}$ and $\bmu{CA\hat{x}}$. This would eliminate the error term outright as it would now be implicitly part of $\bmu{c}$. The derivation would be similar
\begin{equation}
\begin{aligned}
\bmu{V} &= \bmu{\Omega}^T\bmu{B}^T\bmu{C}^T\left(\bmu{y} - \bmu{\hat{y}}\right) - \lambda_d\mu \bmu{r}\\
\bmu{\dot{V}} &= \bmu{\Omega}^T\bmu{B}^T\bmu{C}^T\left(\bmu{\dot{y}} - \bmu{\dot{\hat{y}}}\right) - \lambda_d\mu \bmu{\dot{r}}\\
&= \bmu{\Omega}^T\bmu{B}^T\bmu{C}^T\left(\bmu{\dot{y}} - \bmu{C}\bmu{\dot{\hat{x}}}\right) - \lambda_d\mu \bmu{\dot{r}}\\
&= \bmu{\Omega}^T\bmu{B}^T\bmu{C}^T\left(\bmu{\dot{y}} - \bmu{C}(\bmu{A}\bmu{\hat{x}} + \bmu{B}\bmu{u})\right) - \lambda_d\mu \bmu{\dot{r}}\\
&= \bmu{\Omega}^T\bmu{B}^T\bmu{C}^T\left(\bmu{\dot{y}} - \bmu{C}(\bmu{A}\bmu{\hat{x}} + \bmu{B}\bmu{u})\right) - \mu \lambda_d\left(-\lambda_d\bmu{r} + \lambda_d\bmu{o}\right)\\
&= \bmu{\Omega}^T\bmu{B}^T\bmu{C}^T\left(\bmu{\dot{y}} - \bmu{C}(\bmu{A}\bmu{\hat{x}} + \bmu{B}\left(\frac{1}{\lambda_d}\bmu{\Gamma} \bmu{r} + \bmu{\Omega} \bmu{o}\right)\right) - \lambda_d^2\mu \left(-\bmu{r} + \bmu{o}\right)\\
&= \bmu{\Omega}^T\bmu{B}^T\bmu{C}^T\underbrace{\left(\bmu{\dot{y}} - \bmu{CA\hat{x}}\right)}_{\bmu{c}} + \left(\frac{1}{\lambda_d}\bmu{\Omega}^T\bmu{B}^T\bmu{C}^T\bmu{CB\Gamma} + \mu\lambda_d^2\bmu{I} \right)\bmu{r} \nonumber \\
&\phantom{{}=\bmu{\Omega}^T\bmu{B}^T\bmu{C}^T\underbrace{\left(\bmu{\dot{y}} - \bmu{CA\hat{x}}\right)}_{\bmu{c}}} - \left(\bmu{\Omega}^T\bmu{B}^T\bmu{C}^T\bmu{CB\Omega} - \mu\lambda_d^2\bmu{I} \right)\bmu{o}
\end{aligned}
\end{equation}
with the exception that is now the difference of the reference output and the network output instead of the the state. Otherwise the equation is identical to the original derivation. \todo{Should I keep this in? If so where should I put it?}\\

All this supports the idea that the feed-forward inputs are the governing force in the network's performance. Furthermore by the removal of the trajectory and only the use of its derivative show more the characteristics of an open loop PD controller.\todo{Is that really so?}\\
The benefit of this approach is that given that using this approach we do not need to know the state in order to track any trajectories but just the output or more the derivative of the output\\
that we dont know the state\\
that the C is not square


\section{Results on the learning}

\subsection{Test evaluation}

Explain test methodolofy
What system
what input



\section{Results of the learned control objective}
